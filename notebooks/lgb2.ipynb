{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom imblearn.over_sampling import SMOTE\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))\n    \ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27eb9cc93c1b498c6b451389a336b3837b34ba2b"},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype in [\"int64\"]]\n\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n\n    return df\n\ndef mean_encode(train, val, features_to_encode, target, drop=False):\n    train_encode = train.copy(deep=True)\n    val_encode = val.copy(deep=True)\n    for feature in features_to_encode:\n        train_global_mean = train[target].mean()\n        train_encode_map = pd.DataFrame(index = train[feature].unique())\n        train_encode[feature+'_mean_encode'] = np.nan\n        kf = KFold(n_splits=5, shuffle=False)\n        for rest, this in kf.split(train):\n            train_rest_global_mean = train[target].iloc[rest].mean()\n            encode_map = train.iloc[rest].groupby(feature)[target].mean()\n            encoded_feature = train.iloc[this][feature].map(encode_map).values\n            train_encode[feature+'_mean_encode'].iloc[this] = train[feature].iloc[this].map(encode_map).values\n            train_encode_map = pd.concat((train_encode_map, encode_map), axis=1, sort=False)\n            train_encode_map.fillna(train_rest_global_mean, inplace=True) \n            train_encode[feature+'_mean_encode'].fillna(train_rest_global_mean, inplace=True)\n            \n        train_encode_map['avg'] = train_encode_map.mean(axis=1)\n        val_encode[feature+'_mean_encode'] = val[feature].map(train_encode_map['avg'])\n        val_encode[feature+'_mean_encode'].fillna(train_global_mean,inplace=True)\n        #print('correlation',train[target].corr(train_encode[feature+'_mean_encode']))\n        \n    if drop: #drop unencoded features\n        train_encode.drop(features_to_encode, axis=1, inplace=True)\n        val_encode.drop(features_to_encode, axis=1, inplace=True)\n    return train_encode, val_encode\n\ndef scale_data(df_):\n    df = df_.copy(deep=True)\n    for f_ in df_.columns:\n        if (df[f_].max()- df[f_].min() <=10):\n            df[f_] = df[f_] - df[f_].min()\n            continue\n        df[f_] = df[f_] - df[f_].median()\n        scale = (df[f_].quantile(0.99)-df[f_].quantile(0.01))\n        if scale==0:\n            scale = df[f_].max() - df[f_].min()\n        df[f_] = df[f_]/scale\n        if df[f_].max()>10:\n            rescale = df[f_]>df[f_].quantile(0.99)\n            quantile99 = df[f_].quantile(0.99)\n            quantile100 = df[f_].max()\n            df[f_].loc[rescale] = quantile99 + (df[f_].loc[rescale] - quantile99) * (10-quantile99)/(quantile100-quantile99)\n        if df[f_].min()<-10:\n            rescale = df[f_]<df[f_].quantile(0.01)\n            quantile1 = df[f_].quantile(0.01)\n            quantile0 = df[f_].min()\n            df[f_].loc[rescale] = quantile1 + (df[f_].loc[rescale] - quantile1) * (-10-quantile1)/(quantile0-quantile1)\n        df[f_] = df[f_] - df[f_].min()\n    return df\n\n#from imblearn.over_sampling import SMOTE\ndef smote_nc(X_, y_, nc_feats, nsample_dict, k_neighbors=5):\n    X = X_.copy(deep=True)\n    y = y_.copy(deep=True)\n    nc_dict = {}\n    for f_ in nc_feats:\n        f_oh = pd.get_dummies(X[f_], prefix=f_)\n        nc_dict[f_] = f_oh.columns.values\n        X = pd.concat([X, f_oh], axis=1)\n        del X[f_]\n            \n    sm = SMOTE(random_state=2, k_neighbors=k_neighbors,sampling_strategy=nsample_dict)\n    X_sampled, y_sampled = sm.fit_sample(X,y.values.ravel())\n    X_sampled = pd.DataFrame(X_sampled, columns=X.columns)\n    y_sampled = pd.Series(y_sampled, name=y.name)\n        \n    for f_ in nc_feats:\n        X_sampled[f_] = np.argmax(X_sampled[nc_dict[f_]].values, axis=1)\n        X_sampled.drop(nc_dict[f_], axis=1, inplace=True)\n    \n    int_cols =  [c for c in X_ if X_[c].dtype in [\"int32\",\"int64\"]]\n    for f_ in int_cols:\n        X_sampled[f_] = X_sampled[f_].apply(np.around, decimals=0)\n        \n    return X_sampled, y_sampled\n\ndef sample_feats(excluded_feats, feature_importance_, percent=0.25, baseprob=0.01):\n    feature_importance = feature_importance_.copy(deep=True)\n    feature_importance.importance += baseprob\n    min_importance = feature_importance.importance.quantile(percent)\n    for f_ in feature_importance.feature.loc[feature_importance.importance<min_importance].values:\n        p = np.random.uniform()\n        f_importance = feature_importance.importance.loc[feature_importance.feature==f_]\n        if p > f_importance.any()/min_importance:\n            excluded_feats.append(f_)\n    print ('total number of feature excluded:', len(excluded_feats))\n    print (excluded_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12fb062c4077befd59badd2fc4d01662e67bbfc","collapsed":true},"cell_type":"code","source":"#data = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n#print(data.shape)\n#print(data.columns.values)\n#data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa9b32922310be8c282b77cefa61dbd50c8bf5a5","collapsed":true},"cell_type":"code","source":"meanenc_feats = []\ncat_feats = []\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\ny = data['TARGET']\n#####process train and test together######\ncombined = data.append(test,sort=False)\ntrain_size = data.shape[0]\ntest_size = test.shape[0]\n\ncombined['CODE_GENDER'].replace('XNA', np.nan, inplace=True)\ncombined['NAME_FAMILY_STATUS'].replace('Unknown', np.nan, inplace=True)\ncombined['ORGANIZATION_TYPE'].replace('XNA', np.nan, inplace=True)\ncombined['DAYS_EMPLOYED'].loc[combined['DAYS_EMPLOYED']==365243] = np.nan\ncombined['AMT_INCOME_TOTAL'].clip(upper=combined['AMT_INCOME_TOTAL'].quantile(0.999))\n#create some new features\n#from https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features/code\ndocs = [f_ for f_ in combined.columns if 'FLAG_DOC' in f_]\nlive = [f_ for f_ in combined.columns if ('FLAG_' in f_) & ('FLAG_DOC' not in f_) & ('_FLAG_' not in f_)]\ncombined['NEW_DOC_IND_KURT'] = combined[docs].kurtosis(axis=1)\ncombined['NEW_LIVE_IND_SUM'] = combined[live].sum(axis=1)\ncombined['NEW_INC_PER_CHLD'] = combined['AMT_INCOME_TOTAL'] / (1 + combined['CNT_CHILDREN'])\ninc_by_org = combined[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\ncombined['NEW_INC_BY_ORG'] = combined['ORGANIZATION_TYPE'].map(inc_by_org)\ncombined['NEW_EMPLOY_TO_BIRTH_RATIO'] = combined['DAYS_EMPLOYED'] / combined['DAYS_BIRTH']\ncombined['NEW_SOURCES_PROD'] = combined['EXT_SOURCE_1'] * combined['EXT_SOURCE_2'] * combined['EXT_SOURCE_3']\n#combined['NEW_EXT_SOURCES_MEAN'] = combined[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n#combined['NEW_SCORES_STD'] = combined[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n#combined['NEW_SCORES_STD'] = combined['NEW_SCORES_STD'].fillna(combined['NEW_SCORES_STD'].mean())\n#combined['NEW_CAR_TO_BIRTH_RATIO'] = combined['OWN_CAR_AGE'] / combined['DAYS_BIRTH']\n#combined['NEW_CAR_TO_EMPLOY_RATIO'] = combined['OWN_CAR_AGE'] / combined['DAYS_EMPLOYED']\n#combined['NEW_PHONE_TO_BIRTH_RATIO'] = combined['DAYS_LAST_PHONE_CHANGE'] / combined['DAYS_BIRTH']\n#combined['NEW_PHONE_TO_EMPLOYED_RATIO'] = combined['DAYS_LAST_PHONE_CHANGE'] / combined['DAYS_EMPLOYED']\ncombined['NEW_CREDIT_TO_INCOME_RATIO'] = combined['AMT_CREDIT'] / combined['AMT_INCOME_TOTAL']\n\ncombined['AMT_PAY_YEAR'] = combined['AMT_CREDIT']/combined['AMT_ANNUITY']\ncombined['AGE_PAYOFF'] = -combined['DAYS_BIRTH']/365.25 + combined['AMT_PAY_YEAR']\ncombined['AMT_ANNUITY_INCOME_RATE'] = combined['AMT_ANNUITY']/combined['AMT_INCOME_TOTAL']\ncombined['AMT_DIFF_CREDIT_GOODS'] = combined['AMT_CREDIT'] - combined['AMT_GOODS_PRICE']\ncombined['AMT_CREDIT_GOODS_PERC'] = combined['AMT_CREDIT'] / combined['AMT_GOODS_PRICE']\ncombined['DOCUMENT_CNT'] = combined.loc[:,combined.columns.str.startswith('FLAG_DOCUMENT')].sum(axis=1) #not sure about this\ncombined['AGE_EMPLOYED'] = combined['DAYS_EMPLOYED'] - combined['DAYS_BIRTH']\ncombined['AMT_INCOME_OVER_CHILD'] = combined['AMT_INCOME_TOTAL']/combined['CNT_CHILDREN']\n#combined['CNT_ADULT'] = combined['CNT_FAM_MEMBERS']-combined['CNT_CHILDREN']\n#combined['ADULT_RATIO'] = combined['CNT_ADULT']/combined['CNT_FAM_MEMBERS']\n#combined['AMT_REQ_CREDIT_BUREAU_MON_CHANGE'] = combined['AMT_REQ_CREDIT_BUREAU_QRT']/2 - combined['AMT_REQ_CREDIT_BUREAU_MON']\n#combined['AMT_REQ_CREDIT_BUREAU_QRT_CHANGE'] = combined['AMT_REQ_CREDIT_BUREAU_YEAR']/3 - combined['AMT_REQ_CREDIT_BUREAU_QRT']\ncombined['AMT_REQ_CREDIT_BUREAU_TOTAL'] = combined['AMT_REQ_CREDIT_BUREAU_HOUR'] + combined['AMT_REQ_CREDIT_BUREAU_DAY'] \n+ combined['AMT_REQ_CREDIT_BUREAU_MON'] + combined['AMT_REQ_CREDIT_BUREAU_QRT'] + combined['AMT_REQ_CREDIT_BUREAU_YEAR']\n\ncombined['REGION'] = combined['REGION_POPULATION_RELATIVE'].astype('str') + '_' + combined['REGION_RATING_CLIENT_W_CITY'].astype('str')\n\ncombined['GENDER_FAMILY_STATUS'] = combined['CODE_GENDER'].astype('str') + combined['NAME_FAMILY_STATUS']\ncombined['CNT_CHILDREN_CLIPPED'] = combined['CNT_CHILDREN'].clip(0,8)\n\n#how does clients income compare to ...\ngender_mean_income = combined.groupby('CODE_GENDER')['AMT_INCOME_TOTAL'].mean()\nown_car_mean_income = combined.groupby('FLAG_OWN_CAR')['AMT_INCOME_TOTAL'].mean()\nown_realty_mean_income = combined.groupby('FLAG_OWN_REALTY')['AMT_INCOME_TOTAL'].mean()\ncnt_children_mean_income = combined.groupby('CNT_CHILDREN_CLIPPED')['AMT_INCOME_TOTAL'].mean()\nregion_mean_income = combined.groupby('REGION')['AMT_INCOME_TOTAL'].mean()\nfamily_status_mean_income = combined.groupby('NAME_FAMILY_STATUS')['AMT_INCOME_TOTAL'].mean()\ngender_family_status_mean_income = combined.groupby('GENDER_FAMILY_STATUS')['AMT_INCOME_TOTAL'].mean()\noccupation_mean_income = combined.groupby('OCCUPATION_TYPE')['AMT_INCOME_TOTAL'].mean()\n\ncombined['gender_mean_income'] = combined['CODE_GENDER'].map(gender_mean_income)\ncombined['own_car_mean_income'] = combined['FLAG_OWN_CAR'].map(own_car_mean_income)\ncombined['own_realty_mean_income'] = combined['FLAG_OWN_REALTY'].map(own_realty_mean_income)\ncombined['cnt_children_mean_income'] = combined['CNT_CHILDREN_CLIPPED'].map(cnt_children_mean_income)\ncombined['region_mean_income'] = combined['REGION'].map(region_mean_income)\ncombined['family_status_mean_income'] = combined['NAME_FAMILY_STATUS'].map(family_status_mean_income)\ncombined['gender_family_status_mean_income'] = combined['GENDER_FAMILY_STATUS'].map(gender_family_status_mean_income)\ncombined['occupation_mean_income'] = combined['OCCUPATION_TYPE'].map(occupation_mean_income)\n\ncombined['gender_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['gender_mean_income'])/combined['gender_mean_income']\ncombined['own_car_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['own_car_mean_income'])/combined['own_car_mean_income']\ncombined['own_realty_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['own_realty_mean_income'])/combined['own_realty_mean_income']\ncombined['cnt_children_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['cnt_children_mean_income'])/combined['cnt_children_mean_income']\ncombined['region_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['region_mean_income'])/combined['region_mean_income']\ncombined['family_status_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['family_status_mean_income'])/combined['family_status_mean_income']\ncombined['gender_family_status_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['gender_family_status_mean_income'])/combined['gender_family_status_mean_income']\ncombined['occupation_mean_income_rel'] = (combined['AMT_INCOME_TOTAL']-combined['occupation_mean_income'])/combined['occupation_mean_income']\n\n#data['REGION_W_CITY'] = data['REGION_POPULATION_RELATIVE'].astype('str')+'_'+data['REGION_RATING_CLIENT_W_CITY'].astype('str')\n\n#these features hightly correlated with others\nrejected_features = ['AMT_GOODS_PRICE',\n                     'APARTMENTS_AVG','APARTMENTS_MEDI',\n                     'BASEMENTAREA_AVG','BASEMENTAREA_MODE','COMMONAREA_AVG','COMMONAREA_MODE',\n                     'ELEVATORS_AVG','ELEVATORS_MEDI','ENTRANCES_AVG','ENTRANCES_MEDI','FLOORSMAX_AVG','FLOORSMAX_MEDI',\n                     'FLOORSMIN_AVG','FLOORSMIN_MEDI','LANDAREA_AVG','LANDAREA_MODE',\n                     'LIVINGAPARTMENTS_AVG','LIVINGAPARTMENTS_MEDI',\n                     'LIVINGAREA_AVG','LIVINGAREA_MODE',\n                     'NONLIVINGAPARTMENTS_AVG','NONLIVINGAPARTMENTS_MEDI',\n                     'NONLIVINGAREA_AVG','NONLIVINGAREA_MODE','OBS_60_CNT_SOCIAL_CIRCLE',\n                     'REGION_RATING_CLIENT_W_CITY','YEARS_BEGINEXPLUATATION_AVG','YEARS_BEGINEXPLUATATION_MEDI',\n                     'YEARS_BUILD_AVG','YEARS_BUILD_MEDI']\n#lets see if we can exclude these..\nrejected_featues = rejected_features + ['ELEVATORS_MODE','ENTRANCE_MODE','FLOORSMAX_MEDI','FLOORSMIN_MEDI',\n                    'NONLIVINGAPARTMENTS_MODE',]#'LIVINGAPARTMENTS_MODE',\n#these features are not informative\nrejected_features = rejected_features + ['FLAG_MOBIL','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_2',\n                                        'WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START']\nrejected_features = rejected_features + ['gender_mean_income', 'own_car_mean_income', 'own_realty_mean_income', \n                                         'cnt_children_mean_income', 'family_status_mean_income',\n                                         'gender_family_status_mean_income',\n                                         'CNT_CHILDREN_CLIPPED']\n\nfor f_ in rejected_features:\n    del combined[f_]\n    \n#these features are schewed, band these features\n#'AMT_INCOME_TOTAL','OWN_CAR_AGE','CNT_CHILDREN',\n#'DEF_30_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE\n#'AMT_REQ_CREDIT_BUREAU_DAY/WEEK/MON/QRT/YEAR\n\n#Some feature scaling\n#combined['AMT_INCOME_TOTAL'] = combined['AMT_INCOME_TOTAL'].apply(np.sqrt)/1e2\n#combined['AMT_CREDIT'] = combined['AMT_CREDIT'].apply(np.sqrt)/1e2\n#combined['AMT_ANNUITY'] = combined['AMT_ANNUITY']/2e4\n#combined['DAYS_BIRTH'] = combined['DAYS_BIRTH']/3652\n#combined['DAYS_EMPLOYED'] = (combined['DAYS_EMPLOYED']/1000).clip(upper=10)\n#combined['DAYS_REGISTRATION'] = combined['DAYS_BIRTH']/3652\n#combined['DAYS_ID_PUBLISH'] = combined['DAYS_ID_PUBLISH']/1000\n#combined['OWN_CAR_AGE'] = combined['OWN_CAR_AGE']/10\n#combined['HOUR_APPR_PROCESS_START'] = combined['HOUR_APPR_PROCESS_START']/24\n#combined['EXT_SOURCE_1'] = combined['EXT_SOURCE_1']*10\n#combined['EXT_SOURCE_2'] = combined['EXT_SOURCE_2']*10\n#combined['EXT_SOURCE_3'] = combined['EXT_SOURCE_3']*10\n#combined['DAYS_LAST_PHONE_CHANGE'] = combined['DAYS_LAST_PHONE_CHANGE']/1000\n#combined['AMT_REQ_CREDIT_BUREAU_QRT'] = combined['AMT_REQ_CREDIT_BUREAU_QRT'].clip(upper=19)\n\ndata = combined.iloc[0:train_size,:].copy(deep=True)\ntest = combined.iloc[-test_size:,:].copy(deep=True)\nprint(data.shape,test.shape,combined.shape)\n##### Split train and test######\n\n#Label Encoding\ncategorical_feats = [\n    f for f in data.columns if data[f].dtype == 'object'\n]\n\nfor f_ in categorical_feats:\n    nunique = data[f_].nunique(dropna=False)\n    print(f_,nunique,data[f_].unique())\n    if (nunique<15):\n        cat_feats.append(f_)\n    else:\n        meanenc_feats.append(f_)\n    data[f_], indexer = pd.factorize(data[f_])\n    test[f_] = indexer.get_indexer(test[f_])\n\ndata = downcast_dtypes(data)\ntest = downcast_dtypes(test)\n\ndel combined\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47c8d5a61e24759d8d80505a8a5b97526d2f954b","collapsed":true},"cell_type":"code","source":"data.to_csv('data_app.csv', index=False, compression='zip')\ntest.to_csv('test_app.csv', index=False, compression='zip')\n#data.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5fa41196d1af6a9a7ad6534d76b007725895ca","collapsed":true},"cell_type":"code","source":"#bubl = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\n#bubl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4be18dd8500b8075b19f9f9d44a208cfaee252","collapsed":true},"cell_type":"code","source":"bubl = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\n\n#what is the last month with DPD\nbubl_last_DPD = bubl[bubl.STATUS.isin(['1','2','3','4','5'])].groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].max()\nbubl_last_DPD.rename('MONTH_LAST_DPD', inplace=True)\nprint(bubl_last_DPD.head())\n\n#what is the last month complete\nbubl_last_C = bubl[bubl.STATUS=='C'].groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].min()\nbubl_last_C.rename('MONTH_LAST_C',inplace=True)\nprint(bubl_last_C.head())\n\nSTATUS_TCNT = pd.Series(bubl[bubl['MONTHS_BALANCE']>=-72].groupby('SK_ID_BUREAU')['STATUS'].value_counts(), name='STATUS_TCNT')\nSTATUS_TCNT = pd.pivot_table(STATUS_TCNT.reset_index(),\n                            index='SK_ID_BUREAU',columns='STATUS',values='STATUS_TCNT',fill_value=0)\nSTATUS_TCNT['DPD_SUM'] = np.zeros([STATUS_TCNT.shape[0]])\ncount = np.zeros([STATUS_TCNT.shape[0]])\nfor i in range(0,6):\n    STATUS_TCNT['DPD_SUM'] += STATUS_TCNT[str(i)]*i\n    count += STATUS_TCNT[str(i)]\n    del STATUS_TCNT[str(i)]\nSTATUS_TCNT['DPD_MEAN'] = STATUS_TCNT['DPD_SUM']/(count+0.0001)\n\nSTATUS_TCNT.columns = ['STATUS_TCNT_' + f_ for f_ in STATUS_TCNT.columns]\nSTATUS_24CNT = pd.Series(bubl[bubl['MONTHS_BALANCE']>=-24].groupby('SK_ID_BUREAU')['STATUS'].value_counts(), name='STATUS_6CNT')  \nSTATUS_24CNT = pd.pivot_table(STATUS_24CNT.reset_index(),\n                            index='SK_ID_BUREAU',columns='STATUS',values='STATUS_6CNT',fill_value=0)\nSTATUS_24CNT['DPD_SUM'] = np.zeros([STATUS_24CNT.shape[0]])\ncount = np.zeros([STATUS_24CNT.shape[0]])\nfor i in range(0,6):\n    STATUS_24CNT['DPD_SUM'] += STATUS_24CNT[str(i)]*i\n    count += STATUS_24CNT[str(i)]\n    del STATUS_24CNT[str(i)]\nSTATUS_24CNT['DPD_MEAN'] = STATUS_24CNT['DPD_SUM']/(count+0.0001)\nSTATUS_24CNT.columns = ['STATUS_24CNT_' + f_ for f_ in STATUS_24CNT.columns]\nSTATUS_24CNT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99499bef31c7232aac1f4691cef7f1558b108b34","collapsed":true},"cell_type":"code","source":"#buro = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\n#buro.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05f3c205bbc2ec99010635fe52883ae72bfe336e","collapsed":true},"cell_type":"code","source":"# Now take care of bureau <===peoples credit at different buro\nburo = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\n\nburo['DAYS_CREDIT_ENDDATE'].loc[buro['DAYS_CREDIT_ENDDATE'] < -40000] = np.nan\nburo['DAYS_CREDIT_UPDATE'].loc[buro['DAYS_CREDIT_UPDATE'] < -40000] = np.nan\nburo['DAYS_ENDDATE_FACT'].loc[buro['DAYS_ENDDATE_FACT'] < -40000] = np.nan\n        \nburo['AMT_DEBT_RATIO'] = buro['AMT_CREDIT_SUM_DEBT']/(1+buro['AMT_CREDIT_SUM'])\nburo['AMT_LIMIT_RATIO'] = buro['AMT_CREDIT_SUM_LIMIT']/(1+buro['AMT_CREDIT_SUM'])\nburo['AMT_SUM_OVERDUE_RATIO'] = buro['AMT_CREDIT_SUM_OVERDUE']/(1+buro['AMT_CREDIT_SUM'])\nburo['AMT_MAX_OVERDUE_RATIO'] = buro['AMT_CREDIT_MAX_OVERDUE']/(1+buro['AMT_CREDIT_SUM'])\nburo['DAYS_END_DIFF'] = buro['DAYS_ENDDATE_FACT'] - buro['DAYS_CREDIT_ENDDATE']\n###################################\n# most recent bureau info\n###################################\n\nidx = buro.groupby(['SK_ID_CURR'])['DAYS_CREDIT'].idxmax() #most recent data\nburo_recent = buro.loc[idx.values]\nburo_recent.columns = ['recent_' + f_ for f_ in buro_recent.columns]\n#Label Encoding\ncategorical_feats = [\n    f for f in buro_recent.columns if buro_recent[f].dtype == 'object'\n]\n\nfor f_ in categorical_feats:\n    nunique = buro_recent[f_].nunique(dropna=False)\n    print(f_,nunique,buro_recent[f_].unique())\n    if (nunique>=3):\n        meanenc_feats.append('bureau_'+f_)\n    buro_recent[f_], indexer = pd.factorize(buro_recent[f_])\n\ndel buro_recent['recent_SK_ID_BUREAU']\nburo_recent.rename(columns={'recent_SK_ID_CURR':'SK_ID_CURR'},inplace=True)\nburo_recent.set_index('SK_ID_CURR', inplace=True)\n    \n#### merge buro balance   \nfor f_ in STATUS_TCNT.columns:\n    buro[f_] = buro['SK_ID_BUREAU'].map(STATUS_TCNT[f_])\nfor f_ in STATUS_24CNT.columns:\n    buro[f_] = buro['SK_ID_BUREAU'].map(STATUS_24CNT[f_])\nburo['MONTH_LAST_DPD'] = buro['SK_ID_BUREAU'].map(bubl_last_DPD)\nburo['MONTH_LAST_C'] = buro['SK_ID_BUREAU'].map(bubl_last_C)\n    \n#fill these feature with median\n#buro.DAYS_CREDIT_ENDDATE.fillna(buro.DAYS_CREDIT_ENDDATE.median(),inplace=True)\n#buro.DAYS_ENDDATE_FACT.fillna(buro.DAYS_ENDDATE_FACT.median(),inplace=True)\n\n#one-hot/label encoding categorical feature\nburo_cat_features = [\n    f_ for f_ in buro.columns if buro[f_].dtype == 'object'\n]\nfor f_ in buro_cat_features:\n    # buro[f_], _ = pd.factorize(buro[f_])\n    if(buro[f_].nunique(dropna=False)<=2):\n        buro[f_], indexer = pd.factorize(buro[f_])\n    else:\n        buro = pd.concat([buro, pd.get_dummies(buro[f_], prefix=f_)], axis=1)\n        del buro[f_]\n\n#agg max\nburo['DAYS_CREDIT'] = buro['DAYS_CREDIT']\nmax_feats = ['MONTH_LAST_DPD', 'MONTH_LAST_C', 'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE']\nprint ('max_feats',max_feats)\nmax_buro = buro[max_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').max()\nmax_buro.columns = ['max_' + f_ for f_ in max_buro.columns]\n\n#agg min\nmin_feats = ['MONTH_LAST_DPD', 'MONTH_LAST_C', 'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE']\nprint ('min_feats',min_feats)\nmin_buro = buro[min_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').min()\nmin_buro.columns = ['min_' + f_ for f_ in min_buro.columns]\n\n#compute average\navg_feats = [f_ for f_ in buro.columns.values if (f_.find('DAY')>=0)]\nprint ('avg_feats',avg_feats)\navg_buro = buro[avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').median()\navg_buro.columns = ['avg_' + f_ for f_ in avg_buro.columns]\n\nsum_feats = [f_ for f_ in buro.columns.values if not f_ in (['SK_ID_CURR','SK_ID_BUREAU'])]\nsum_buro = buro[sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\nsum_buro.columns = ['sum_' + f_ for f_ in sum_buro.columns]\nprint ('sum_feats',sum_feats)\n\nfor cat_ in buro_cat_features:\n    print('buro_'+cat_+'_mode')\n    cols = [f_ for f_ in sum_buro.columns.values if f_.find(cat_)>=0]\n    sum_buro[cat_+'_mode'] = sum_buro[cols].idxmax(axis=1)\n    sum_buro[cat_+'_mode'], indexer = pd.factorize(sum_buro[cat_+'_mode'])\n    meanenc_feats.append('bureau_'+cat_+'_mode')\n    if len(cols)>=10:\n        for col in cols:\n            del sum_buro[col]\n            \nactive_buro = buro.loc[buro['CREDIT_ACTIVE_Active']==1]\nactive_buro['DAYS_LEFT_RATIO'] = active_buro['DAYS_CREDIT_ENDDATE']/(active_buro['DAYS_CREDIT_ENDDATE']-active_buro['DAYS_CREDIT'])\nactive_buro['AMT_CREDIT_LEFT'] = active_buro['AMT_CREDIT_SUM'] * active_buro['DAYS_LEFT_RATIO']\nactive_buro['AMT_CREDIT_LEFT_OVER_ANNUITY'] = active_buro['AMT_CREDIT_LEFT'] / active_buro['AMT_ANNUITY']\nactive_sum_feats = [f_ for f_ in sum_feats if (f_.find('CREDIT_CURRENCY')<0)\n                    & (f_.find('CREDIT_ACTIVE')<0) & (f_.find('STATUS_')<0)\n                    & (f_.find('MONTH_')<0) & (f_.find('CREDIT_TYPE')<0)] + ['AMT_CREDIT_LEFT','AMT_CREDIT_LEFT_OVER_ANNUITY']\nprint ('active_sum_feats',active_sum_feats)\nactive_sum_buro = active_buro[active_sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\ndel active_sum_buro['DAYS_END_DIFF']\nactive_sum_buro.columns = ['active_sum_' + f_ for f_ in active_sum_buro.columns]\nactive_sum_buro['active_count'] = buro.loc[buro['CREDIT_ACTIVE_Active']==1].groupby('SK_ID_CURR')['SK_ID_BUREAU'].nunique()\n\nactive_avg_feats = active_sum_feats + ['DAYS_LEFT_RATIO']\nprint ('active_avg_feats',active_avg_feats)\nactive_avg_buro = active_buro[active_avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').median()\ndel active_avg_buro['DAYS_END_DIFF']\nactive_avg_buro.columns = ['active_avg_' + f_ for f_ in active_avg_buro.columns] \n\navg_buro = avg_buro.merge(min_buro, how='outer', on='SK_ID_CURR')\navg_buro = avg_buro.merge(max_buro, how='outer', on='SK_ID_CURR')\navg_buro = avg_buro.merge(sum_buro, how='outer', on='SK_ID_CURR')\navg_buro = avg_buro.merge(active_sum_buro, how='outer', on='SK_ID_CURR')\navg_buro = avg_buro.merge(active_avg_buro, how='outer', on='SK_ID_CURR')\navg_buro = avg_buro.merge(buro_recent, how='outer', on='SK_ID_CURR')\navg_buro['count'] = buro.groupby('SK_ID_CURR')['SK_ID_BUREAU'].nunique()\n#del avg_buro['SK_ID_BUREAU']\n\navg_buro.columns = ['bureau_' + f_ for f_ in avg_buro.columns]\n#downcast to save space\navg_buro = downcast_dtypes(avg_buro)\n\ndel buro, sum_feats, active_sum_buro, bubl, STATUS_TCNT, STATUS_24CNT\ngc.collect()\navg_buro.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1abfb9052d3d67c295e6080af4d1af8cb07d915","collapsed":true},"cell_type":"code","source":"avg_buro.to_csv('avg_buro.csv', compression='zip')\n#avg_buro.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b6bbe8f4a91e5068abc3a3fad5c4db681d6820a","collapsed":true},"cell_type":"code","source":"#avg_buro = scale_data(avg_buro)\n#avg_buro.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45297772b7a5a7429f4e35755f76f1ec41b9bc7","collapsed":true},"cell_type":"code","source":"#ccbl = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\n#ccbl.head()\n#pred_feats = [f_ for f_ in ccbl.columns.values if ((f_.find('AMT')>=0) | (f_.find('CNT')>=0) & (f_.find('CUM')==-1))]\n#tmp = ccbl.groupby(['SK_ID_CURR','MONTHS_BALANCE'])[pred_feats].sum()\n#tmp = tmp.reset_index()\n#table = pd.pivot_table(tmp, index='SK_ID_CURR', columns='MONTHS_BALANCE', values=pred_feats, fill_value=0)\n#table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8002028c2bd5637f570c22fe33e1ef45a68e9928","collapsed":true},"cell_type":"markdown","source":"ccbl_fit = pd.read_csv('../input/credit-card-balance-time-series/ccbl_fit.csv',index_col=0)\nccbl_fit.index.rename('SK_ID_CURR',inplace=True)\n#ccbl_fit.describe()\nuse_cols = ['AMT_INST_MIN_REGULARITY_k',\n            'AMT_PAYMENT_CURRENT_k', \n            'AMT_CREDIT_USE_RATIO_k',\n            'AMT_DRAWING_ATM_RATIO_k',\n            'AMT_PAY_USE_RATIO_k']\nccbl_fit = ccbl_fit[use_cols]\nccbl_fit.head()"},{"metadata":{"trusted":true,"_uuid":"73cd8532460a9502c3cc4d1a953b785b3ad398d9","collapsed":true},"cell_type":"code","source":"ccbl = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\ncc_target1 = ccbl.SK_ID_PREV.loc[ccbl.SK_DPD_DEF>0].unique()\n\nsum_feats = [f_ for f_ in ccbl.columns.values if ((f_.find('AMT')>=0) | (f_.find('SK_DPD')>=0) | (f_.find('CNT')>=0) & (f_.find('CUM')==-1))]\nprint('sum_feats',sum_feats)\nsum_ccbl_mon = ccbl.groupby(['SK_ID_CURR','MONTHS_BALANCE'])[sum_feats].sum()\nsum_ccbl_mon['CNT_ACCOUNT_W_MONTH'] = ccbl.groupby(['SK_ID_CURR','MONTHS_BALANCE'])['SK_ID_PREV'].count()\nsum_ccbl_mon = sum_ccbl_mon.reset_index()\n\n#compute ratio after summing up account\nsum_ccbl_mon['AMT_BALANCE_CREDIT_RATIO'] = (sum_ccbl_mon['AMT_BALANCE']/(sum_ccbl_mon['AMT_CREDIT_LIMIT_ACTUAL']+0.001)).clip(-100,100)\nsum_ccbl_mon['AMT_CREDIT_USE_RATIO'] = (sum_ccbl_mon['AMT_DRAWINGS_CURRENT']/(sum_ccbl_mon['AMT_CREDIT_LIMIT_ACTUAL']+0.001)).clip(-100,100)\nsum_ccbl_mon['AMT_DRAWING_ATM_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_ATM_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\nsum_ccbl_mon['AMT_DRAWINGS_OTHER_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_OTHER_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\nsum_ccbl_mon['AMT_DRAWINGS_POS_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_POS_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\nsum_ccbl_mon['AMT_PAY_USE_RATIO'] = ((sum_ccbl_mon['AMT_PAYMENT_TOTAL_CURRENT']+0.001)/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)).clip(-100,100)\nsum_ccbl_mon['AMT_BALANCE_RECIVABLE_RATIO'] = sum_ccbl_mon['AMT_BALANCE']/(sum_ccbl_mon['AMT_TOTAL_RECEIVABLE']+0.001)\nsum_ccbl_mon['AMT_DRAWING_BALANCE_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_CURRENT']/(sum_ccbl_mon['AMT_BALANCE']+0.001)\nsum_ccbl_mon['AMT_RECEIVABLE_PRINCIPAL_DIFF'] = sum_ccbl_mon['AMT_TOTAL_RECEIVABLE']-sum_ccbl_mon['AMT_RECEIVABLE_PRINCIPAL']\nsum_ccbl_mon['AMT_PAY_INST_DIFF'] = sum_ccbl_mon['AMT_PAYMENT_CURRENT'] - sum_ccbl_mon['AMT_INST_MIN_REGULARITY']\n\nrejected_features = ['AMT_RECIVABLE','AMT_RECEIVABLE_PRINCIPAL',\n                     'AMT_DRAWINGS_OTHER_CURRENT','AMT_DRAWINGS_POS_CURRENT']\nfor f_ in rejected_features:\n    del sum_ccbl_mon[f_]\n    \nsum_feats = [f_ for f_ in sum_ccbl_mon.columns.values if ((f_.find('AMT')>=0) | (f_.find('SK_DPD')>=0) | (f_.find('CNT')>=0) & (f_.find('CUM')==-1))]\nprint('updated sum_feats',sum_feats)\n#mean1_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-1].groupby('SK_ID_CURR').mean()\n#del mean1_ccbl_mon['MONTHS_BALANCE']\n#mean1_ccbl_mon.columns = ['mean1_' + f_ for f_ in mean1_ccbl_mon.columns]\n\nprint('compute mean for different windows')\nmean4_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-6].groupby('SK_ID_CURR').mean()\ndel mean4_ccbl_mon['MONTHS_BALANCE']\nmean4_ccbl_mon.columns = ['mean4_' + f_ for f_ in mean4_ccbl_mon.columns]\n\nmean12_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-24].groupby('SK_ID_CURR').mean()\ndel mean12_ccbl_mon['MONTHS_BALANCE']\nmean12_ccbl_mon.columns = ['mean12_' + f_ for f_ in mean12_ccbl_mon.columns]\n\n#mean36_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-36].groupby('SK_ID_CURR').mean()\n#del mean36_ccbl_mon['MONTHS_BALANCE']\n#mean36_ccbl_mon.columns = ['mean36_' + f_ for f_ in mean36_ccbl_mon.columns]\n\n#sum_ccbl_mon2 for scale features\nprint('compute scaled sum and mean')\nsum_ccbl_mon2 = sum_ccbl_mon.copy(deep=True)\nsum_ccbl_mon2['YEAR_SCALE'] = (-12/sum_ccbl_mon2['MONTHS_BALANCE']).clip(upper=2)\nfor f_ in sum_feats:\n    sum_ccbl_mon2[f_] = sum_ccbl_mon2[f_] * sum_ccbl_mon2['YEAR_SCALE']\n\n#scale sum\nscale_sum_ccbl_mon = sum_ccbl_mon2.groupby('SK_ID_CURR').sum()\ndel scale_sum_ccbl_mon['MONTHS_BALANCE'], scale_sum_ccbl_mon['YEAR_SCALE']\nscale_sum_ccbl_mon.columns = ['scale_sum_' + f_ for f_ in scale_sum_ccbl_mon.columns]\n\n#scale mean\nyear_scale_sum = sum_ccbl_mon2.groupby('SK_ID_CURR')['YEAR_SCALE'].sum()\nscale_mean_ccbl_mon = pd.DataFrame()\nfor f_ in scale_sum_ccbl_mon.columns:\n    scale_mean_ccbl_mon[f_] = scale_sum_ccbl_mon[f_]/year_scale_sum\nscale_mean_ccbl_mon.columns = ['scale_mean_' + f_ for f_ in scale_mean_ccbl_mon.columns]\n\nprint ('compute mean,var,max,min for all months')\n#mean\n#del sum_ccbl_mon['MONTHS_BALANCE']\nmean_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR').median()\ndel mean_ccbl_mon['MONTHS_BALANCE']\nmean_ccbl_mon.columns = ['mean_' + f_ for f_ in mean_ccbl_mon.columns]\n#var\nvar_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR').var()\ndel var_ccbl_mon['MONTHS_BALANCE']\nvar_ccbl_mon.columns = ['var_' + f_ for f_ in var_ccbl_mon.columns]\n#max\nmax_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon['MONTHS_BALANCE']>-60].groupby('SK_ID_CURR').max()\nmax_ccbl_mon.columns = ['max_' + f_ for f_ in max_ccbl_mon.columns]\n#min\nmin_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon['MONTHS_BALANCE']>-60].groupby('SK_ID_CURR')['AMT_TOTAL_RECEIVABLE','AMT_RECEIVABLE_PRINCIPAL_DIFF'].min()\nmin_ccbl_mon.columns = ['min_' + f_ for f_ in min_ccbl_mon.columns]\n\nprint ('find last time with DPD')\n#what is the last month with DPD\nccbl_last_DPD = ccbl[ccbl.SK_DPD>0].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\nccbl_last_DPD.rename('MONTH_LAST_DPD',inplace=True)\n\n#what is the last month with 7 Days Past Due\nccbl_last_DPD7 = ccbl[ccbl.SK_DPD_DEF>7].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\nccbl_last_DPD7.rename('MONTH_LAST_DPD7',inplace=True)\n\n#ccbl_mon = mean1_ccbl_mon.merge(mean4_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = mean4_ccbl_mon.copy(deep=True)\nccbl_mon = ccbl_mon.merge(mean12_ccbl_mon,how='outer',on='SK_ID_CURR')\n#ccbl_mon = ccbl_mon.merge(mean36_ccbl_mon,how='outer',on='SK_ID_CURR')\n\nccbl_mon = ccbl_mon.merge(scale_sum_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(scale_mean_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(mean_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(var_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(max_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(min_ccbl_mon,how='outer',on='SK_ID_CURR')\nccbl_mon['MONTH_LAST_DPD'] = ccbl_last_DPD\nccbl_mon['MONTH_LAST_DPD7'] = ccbl_last_DPD7\nccbl_mon['MONTH_LAST_DPD'].loc[ccbl_mon['MONTH_LAST_DPD']==0] = np.nan\nccbl_mon['MONTH_LAST_DPD7'].loc[ccbl_mon['MONTH_LAST_DPD7']==0] = np.nan\n\n#most recent data\nprint ('extract most recent data for each customer')\nidx = ccbl.groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].idxmax()\nrecent = ccbl[['SK_ID_CURR','MONTHS_BALANCE','CNT_INSTALMENT_MATURE_CUM','NAME_CONTRACT_STATUS','SK_DPD','SK_DPD_DEF']].iloc[idx.values].copy(deep=True)\n#most recent NAME_CONTRACT_STATUS for mean encoding\nrecent['NAME_CONTRACT_STATUS'],indexer = pd.factorize(recent['NAME_CONTRACT_STATUS'])\nmeanenc_feats.append('cc_NAME_CONTRACT_STATUS')\nrecent.set_index('SK_ID_CURR',inplace=True)\n\nNAME_CONTRACT_STATUS_COUNT = pd.Series(ccbl.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].value_counts(),\n                                       name='NAME_CONTRACT_STATUS_COUNT')\nNAME_CONTRACT_STATUS_COUNT = pd.pivot_table(NAME_CONTRACT_STATUS_COUNT.reset_index(), \n               index='SK_ID_CURR', columns='NAME_CONTRACT_STATUS', values='NAME_CONTRACT_STATUS_COUNT',fill_value=0)\nrecent = recent.merge(NAME_CONTRACT_STATUS_COUNT,how='outer',on='SK_ID_CURR')\nccbl_mon = ccbl_mon.merge(recent,how='outer',on='SK_ID_CURR')\nccbl['history_len'] = ccbl.groupby('SK_ID_CURR')['MONTHS_BALANCE'].count()\n\n#########\nccbl_mon.fillna(0,inplace=True)\nccbl_mon.columns = ['cc_' + f_ for f_ in ccbl_mon.columns]\nccbl_mon = downcast_dtypes(ccbl_mon)\n\ndel sum_ccbl_mon, sum_ccbl_mon2\n#del mean1_ccbl_mon\ndel mean4_ccbl_mon, mean12_ccbl_mon, recent\ndel scale_sum_ccbl_mon, scale_mean_ccbl_mon, mean_ccbl_mon, var_ccbl_mon, max_ccbl_mon\ndel ccbl\ngc.collect()\nccbl_mon.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07b5b665d951d3d174082d647be57417d00a9ffe","collapsed":true},"cell_type":"code","source":"ccbl_mon.to_csv('ccbl_mon.csv', compression='zip')\n#ccbl_mon = scale_data(ccbl_mon)\n#ccbl_mon.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158afe295f41cfb1d5b34eda594df4cee3ccc576","collapsed":true},"cell_type":"code","source":"#pos = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\n#pos.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc9a1d2becaa76d4a98c548b41775563e2e91e2c","collapsed":true},"cell_type":"code","source":"pos = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\npos_target1 = pos.SK_ID_PREV.loc[pos.SK_DPD_DEF>0].unique()\n\n#later use with prev\nidx = pos.groupby(['SK_ID_PREV'])['MONTHS_BALANCE'].idxmax() #most recent data\npos_prev_last = pos[['SK_ID_PREV','CNT_INSTALMENT','CNT_INSTALMENT_FUTURE']].loc[idx.values]\npos_prev_last['INSTAL_LEFT_RATIO'] = pos_prev_last['CNT_INSTALMENT_FUTURE']/(pos_prev_last['CNT_INSTALMENT'])\npos_prev_last.set_index('SK_ID_PREV',inplace=True)\n#####\n\nidx = pos.groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].idxmax() #most recent data\npos_recent = pos[['SK_ID_CURR','MONTHS_BALANCE','CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n                  'NAME_CONTRACT_STATUS','SK_DPD','SK_DPD_DEF']].loc[idx.values]\npos_recent['NAME_CONTRACT_STATUS'],indexer = pd.factorize(pos_recent['NAME_CONTRACT_STATUS'])\npos_recent.set_index('SK_ID_CURR',inplace=True)\npos_recent.columns = ['recent_' + f_ for f_ in pos_recent.columns]\n\nNAME_CONTRACT_STATUS_COUNT = pd.Series(pos.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].value_counts(),\n                                       name='NAME_CONTRACT_STATUS_COUNT')\nNAME_CONTRACT_STATUS_COUNT = pd.pivot_table(NAME_CONTRACT_STATUS_COUNT.reset_index(), \n               index='SK_ID_CURR', columns='NAME_CONTRACT_STATUS', values='NAME_CONTRACT_STATUS_COUNT',fill_value=0)\nNAME_CONTRACT_STATUS_COUNT.columns = ['NAME_CONTRACT_STATUS_CNT_' + f_ for f_ in NAME_CONTRACT_STATUS_COUNT.columns] \n\n## aggragate features\n#pos['YEAR_SCALE'] = (pos['MONTHS_BALANCE']/12.0).apply(np.exp)\npos['YEAR_SCALE'] = (-12/pos['MONTHS_BALANCE']).clip(upper=1)\npos['SK_DPD_SCALE'] = pos['SK_DPD'] * pos['YEAR_SCALE']\npos['SK_DPD_DEF_SCALE'] = pos['SK_DPD_DEF'] * pos['YEAR_SCALE']\n\npos_max = pos.loc[pos['MONTHS_BALANCE']>-60].groupby(['SK_ID_CURR'])[['SK_DPD','SK_DPD_DEF']].max()\npos_max.columns = ['max_' + f_ for f_ in pos_max.columns]\n\npos_mean = pos.groupby(['SK_ID_CURR'])[['SK_DPD','SK_DPD_DEF']].median()\npos_mean.columns = ['mean_' + f_ for f_ in pos_mean.columns]\n\npos_sum = pos.groupby(['SK_ID_CURR'])[['SK_DPD_SCALE','SK_DPD_DEF_SCALE']].sum()\n\npos_year_sum = pos.groupby(['SK_ID_CURR'])['YEAR_SCALE'].sum()\npos_mean_scale = pd.DataFrame()\nfor f_ in pos_sum.columns:\n    pos_mean_scale[f_] = pos_sum[f_]/pos_year_sum\n\npos_sum.columns = ['sum_' + f_ for f_ in pos_sum.columns]\npos_mean_scale.columns = ['mean_' + f_ for f_ in pos_mean_scale.columns]\n\n#what is the last month with DPD\npos_last_DPD = pos[pos.SK_DPD>0].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\npos_last_DPD.rename('MONTH_LAST_DPD',inplace=True)\n\npos_recent = pos_recent.merge(pos_max,how='outer',on='SK_ID_CURR')\npos_recent = pos_recent.merge(pos_mean,how='outer',on='SK_ID_CURR')\npos_recent = pos_recent.merge(pos_sum,how='outer',on='SK_ID_CURR')\npos_recent = pos_recent.merge(pos_mean_scale,how='outer',on='SK_ID_CURR')\npos_recent['MONTH_LAST_DPD'] = pos_last_DPD\npos_recent = pos_recent.merge(NAME_CONTRACT_STATUS_COUNT,how='outer',on='SK_ID_CURR')\npos_recent['MONTH_CNT'] = pos.groupby('SK_ID_CURR')['MONTHS_BALANCE'].count()\npos_recent['MONTH_MAX'] = pos.groupby('SK_ID_CURR')['MONTHS_BALANCE'].min()\npos_recent['count'] = pos.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\n\npos_recent.fillna(0,inplace=True)\npos_recent = downcast_dtypes(pos_recent)\npos_recent.columns = ['pos_' + f_ for f_ in pos_recent.columns]\n\nmeanenc_feats.append('pos_recent_NAME_CONTRACT_STATUS')\ndel pos, pos_max, pos_mean, pos_sum, pos_mean_scale\ngc.collect()\npos_recent.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbd1cbad91387455ba1fd3db1571646eb6cdfde7","collapsed":true},"cell_type":"code","source":"pos_recent.to_csv('pos_recent.csv', compression='zip')\n#pos_recent.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2403270616ef58ced6d853c7e5be5c6489c3eb59","collapsed":true},"cell_type":"code","source":"#pos_recent = scale_data(pos_recent)\n#pos_recent.describe(include='all',percentiles=[0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd603329e8614fc88c200d5e9bd449e182b24f29","collapsed":true},"cell_type":"code","source":"inst = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\n\n#later use with prev\ninst_prev_last = inst.groupby('SK_ID_PREV')['AMT_PAYMENT'].sum()\n####\n\ninst_NUM_INSTALMENT_VERSION = inst.groupby(['SK_ID_CURR'])['NUM_INSTALMENT_VERSION'].nunique()\n\n#merge payments of same month\n#maybe helpful for: inst.loc[(inst.SK_ID_PREV==1000005) & (inst.SK_ID_CURR==176456) & (inst.NUM_INSTALMENT_NUMBER==9)]\ninst['DAYS_ENTRY_PAYMENT_weighted'] = inst['DAYS_ENTRY_PAYMENT'] * inst['AMT_PAYMENT']\ninst = inst.groupby(['SK_ID_PREV','SK_ID_CURR','NUM_INSTALMENT_NUMBER']).agg({'DAYS_INSTALMENT':'mean',\n                                                                       'DAYS_ENTRY_PAYMENT_weighted':'sum',\n                                                                       'AMT_INSTALMENT':'mean',\n                                                                       'AMT_PAYMENT':'sum'})\ninst['DAYS_ENTRY_PAYMENT'] = inst['DAYS_ENTRY_PAYMENT_weighted']/inst['AMT_PAYMENT']\ninst = inst.reset_index()\ndel inst['DAYS_ENTRY_PAYMENT_weighted']\n\ninst_target1 = inst.loc[(inst['DAYS_ENTRY_PAYMENT']>inst['DAYS_INSTALMENT']+1)|(inst['AMT_PAYMENT']<inst['AMT_INSTALMENT'])].SK_ID_PREV.unique()\n\n#create some new feature: how many days payment delayed? how much overpayed/underpayed?\ninst['AMT_PAYMENT_PERC'] = inst['AMT_PAYMENT'] / inst['AMT_INSTALMENT']\ninst['DPD'] = inst['DAYS_ENTRY_PAYMENT'] - inst['DAYS_INSTALMENT']\ninst['DBD'] = inst['DAYS_INSTALMENT'] - inst['DAYS_ENTRY_PAYMENT']\ninst['DPD'] = inst['DPD'].apply(lambda x: x if x > 0 else 0)\ninst['DBD'] = inst['DBD'].apply(lambda x: x if x > 0 else 0)\ninst['DPD'].fillna(30, inplace=True)\ninst['DBD'].fillna(0, inplace=True)\ninst['AMT_PAYMENT_DIFF'] = inst['AMT_INSTALMENT'] - inst['AMT_PAYMENT']\ninst['DAYS_ENTRY_PAYMENT_SCALE'] = (-365.25/inst['DAYS_ENTRY_PAYMENT']).clip(upper=1)\ninst['DPD_SCALE'] = inst['DPD'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\ninst['DBD_SCALE'] = inst['DBD'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\ninst['AMT_PAYMENT_DIFF_SCALE'] = inst['AMT_PAYMENT_DIFF'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\ninst['AMT_PAYMENT_SCALE'] = inst['AMT_PAYMENT'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\n\ninst_max = inst.loc[inst['DAYS_ENTRY_PAYMENT']>-365.25*5].groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT_PERC']].max()\ninst_max.columns = ['max_' + f_ for f_ in inst_max.columns]\n\ninst_var = inst.loc[inst['DAYS_ENTRY_PAYMENT']>-365.25*5].groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT_PERC']].var()\ninst_var.columns = ['var_' + f_ for f_ in inst_var.columns]\n\ninst_sum = inst.groupby('SK_ID_CURR')[['DPD_SCALE','DBD_SCALE','AMT_PAYMENT_DIFF_SCALE','AMT_PAYMENT_SCALE']].sum()\n\ninst_day_scale_sum = inst.groupby('SK_ID_CURR')['DAYS_ENTRY_PAYMENT_SCALE'].sum()\ninst_avg_scale = pd.DataFrame()\nfor f_ in inst_sum.columns:\n    inst_avg_scale[f_] = inst_sum[f_]/inst_day_scale_sum\n    \ninst_sum.columns = ['sum_' + f_ for f_ in inst_sum.columns]\ninst_avg_scale.columns = ['mean_' + f_ for f_ in inst_avg_scale.columns]\n\ninst_avg = inst.groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT','AMT_PAYMENT_PERC']].median()\ninst_avg.columns = ['mean_' + f_ for f_ in inst_avg.columns]\n\n#when is the last time late\ninst_last_late = inst[inst.DAYS_INSTALMENT < inst.DAYS_ENTRY_PAYMENT].groupby(['SK_ID_CURR'])['DAYS_INSTALMENT'].max()\ninst_last_late.rename('DAYS_LAST_LATE',inplace=True)\n\n#when is the last time underpaid\ninst_last_underpaid = inst[inst.AMT_INSTALMENT < inst.AMT_PAYMENT].groupby(['SK_ID_CURR'])['DAYS_INSTALMENT'].max()\ninst_last_underpaid.rename('DAYS_LAST_UNDERPAID',inplace=True)\n\n#inst_mean_diff = pd.read_csv('../input/inst-time-series/mean_diff_df.csv', index_col=0, usecols = ['SK_ID_CURR','AMT_PAYMENT_PERC_diff_weighted','AMT_PAYMENT_DIFF_diff_weighted','DPD_diff_weighted','DBD_diff_weighted'])\n\n#inst_avg = inst_avg.merge(inst_mean_diff, on='SK_ID_CURR', how='outer')\ninst_avg = inst_avg.merge(inst_max, on='SK_ID_CURR', how='outer')\ninst_avg = inst_avg.merge(inst_var, on='SK_ID_CURR', how='outer')\ninst_avg = inst_avg.merge(inst_sum, on='SK_ID_CURR', how='outer')\ninst_avg = inst_avg.merge(inst_avg_scale, on='SK_ID_CURR', how='outer')\ninst_avg['DAYS_LAST_LATE'] = inst_last_late\ninst_avg['DAYS_LAST_UNDERPAID'] = inst_last_underpaid\ninst_avg['N_NUM_INSTALMENT_VERSION'] = inst_NUM_INSTALMENT_VERSION\n\ninst_avg['length'] = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_avg['count'] = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\ninst_avg.columns = ['inst_' + f_ for f_ in inst_avg.columns]\ninst_avg = downcast_dtypes(inst_avg)\n\n#avg_inst.fillna(0,inplace=True) #not many nan, should be fine...\ndel inst, inst_sum, inst_max, inst_var\ngc.collect()\ninst_avg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c1b64dc28894232f38b9346418d497a57ac5cf0"},"cell_type":"code","source":"inst_avg.to_csv('inst_avg.csv', compression='zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9374d4c3d2428694e063be15c492bc16ef916b89","collapsed":true},"cell_type":"code","source":"#prev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\n#prev.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7012b1646b1a71b9cda91d02c74d93387c33d12","collapsed":true},"cell_type":"code","source":"prev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\n\nprev = prev.loc[prev['FLAG_LAST_APPL_PER_CONTRACT']=='Y'] #mistake rows\ndel prev['FLAG_LAST_APPL_PER_CONTRACT']\n\nfor f_ in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n    prev[f_].loc[prev[f_]>360000] = np.nan\n\n#create some features\nprev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\nprev['AMT_DIFF_CREAPP'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\nprev['AMT_DIFF_CREDIT_GOODS'] = prev['AMT_CREDIT'] - prev['AMT_GOODS_PRICE']\nprev['AMT_CREDIT_GOODS_PERC'] = prev['AMT_CREDIT'] / prev['AMT_GOODS_PRICE']\nprev['AMT_PAY_YEAR'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\nprev['DAYS_TOTAL'] = prev['DAYS_LAST_DUE'] - prev['DAYS_FIRST_DUE']\nprev['DAYS_TOTAL2'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_FIRST_DUE']\nprev['DAYS_END_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\nprev['CNT_PAYMENT_DIFF'] = prev['CNT_PAYMENT'] - prev['SK_ID_PREV'].map(pos_prev_last['CNT_INSTALMENT'])\n\nprev['DEFAULTED'] = 0\nprev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(inst_target1)] = 1\nprev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(pos_target1)] = 1\nprev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(cc_target1)] = 1\nprev['DEFAULTED'].loc[prev['NAME_CONTRACT_STATUS']!='Approved'] = np.nan\n\n#these features highly correlated with others or not useful?\nrejected_features = ['AMT_GOODS_PRICE',\n                     'WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START',\n                     'NFLAG_LAST_APPL_IN_DAY']\nfor f_ in rejected_features:\n    del prev[f_]\n    \n\n#find number of unique before one-hot\n#nunique_feats = ['SELLERPLACE_AREA', 'NAME_TYPE_SUITE', 'NAME_GOODS_CATEGORY']\n#print ('nunique_feats',nunique_feats)\n#nunique_prev = prev[nunique_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').nunique()\n#nunique_prev.columns = ['nunique_' + f_ for f_ in nunique_prev.columns]\n    \n\n###################################\n# most recent application\n###################################\n#prev.SELLERPLACE_AREA.loc[prev.SELLERPLACE_AREA>0] = prev.SELLERPLACE_AREA.loc[prev.SELLERPLACE_AREA>0].apply(np.log)\n#prev.AMT_DOWN_PAYMENT = prev.AMT_DOWN_PAYMENT.apply(np.log1p)\n\nidx = prev.groupby(['SK_ID_CURR'])['DAYS_DECISION'].idxmax() #most recent data\nprev_recent = prev.loc[idx.values]\nprev_recent.columns = ['recent_' + f_ for f_ in prev_recent.columns]\n#Label Encoding\ncategorical_feats = [\n    f for f in prev_recent.columns if prev_recent[f].dtype == 'object'\n]\n\nfor f_ in categorical_feats:\n    nunique = prev_recent[f_].nunique(dropna=False)\n    print(f_,nunique,prev_recent[f_].unique())\n    if (nunique<10):\n        cat_feats.append('prev_'+f_)\n    else:\n        meanenc_feats.append('prev_'+f_)\n    prev_recent[f_], indexer = pd.factorize(prev_recent[f_])\n\ndel prev_recent['recent_SK_ID_PREV']\nprev_recent.rename(columns={'recent_SK_ID_CURR':'SK_ID_CURR'},inplace=True)\nprev_recent.set_index('SK_ID_CURR', inplace=True)\n\n#when is the last time refused\n#idx = prev[prev.NAME_CONTRACT_STATUS=='Refused'].groupby(['SK_ID_CURR'])['DAYS_DECISION'].idxmax() #most recent data\n#prev_last_refused = prev[['SK_ID_CURR','DAYS_DECISION']].loc[idx.values]\n#prev_last_refused.rename({'DAYS_DECISION':'DAYS_LAST_REFUSED'},axis='columns',inplace=True)\n#prev_last_refused.set_index('SK_ID_CURR',inplace=True)\n#print(prev_last_refused.head())\n\n#when is the last time approved\n#idx = prev[prev.NAME_CONTRACT_STATUS=='Approved'].groupby(['SK_ID_CURR'])['DAYS_DECISION'].idxmax() #most recent data\n#prev_last_approved = prev[['SK_ID_CURR','DAYS_DECISION']].loc[idx.values]\n#prev_last_approved.rename({'DAYS_DECISION':'DAYS_LAST_approved'},axis='columns',inplace=True)\n#prev_last_approved.set_index('SK_ID_CURR',inplace=True)\n#print(prev_last_approved.head())\n\n    \n###################################\n# Changed categorical feature treatment to dummies\n# In this way averaging means something\n################################### \nprev_cat_features = [\n    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n]\nfor f_ in prev_cat_features:\n    if(prev[f_].nunique(dropna=False)<=2):\n        prev[f_], indexer = pd.factorize(prev[f_])\n    else:\n        prev = pd.concat([prev, pd.get_dummies(prev[f_], prefix=f_)], axis=1)\n        del prev[f_]\n################################### \n\navg_feats = [f_ for f_ in prev.columns.values if (f_.find('DAYS')>=0) | (f_.find('RATE')>=0) | (f_.find('AMT')>=0)]\nprint ('avg_feats',avg_feats)\nfor f_ in avg_feats:\n    prev[f_].loc[prev[f_]>300000] = np.nan\navg_prev = prev[avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').median()\navg_prev.columns = ['avg_' + f_ for f_ in avg_prev.columns]\n\nmax_feats = [f_ for f_ in prev.columns.values if (f_.find('DAYS')>=0) | (f_.find('AMT')>=0)]\nprint('max_feats',max_feats)\nmax_prev = prev[max_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').max()\nmax_prev.columns = ['max_' + f_ for f_ in max_prev.columns]\n\nmin_feats = ['DAYS_DECISION']\nprint('min_feats',min_feats)\nmin_prev = prev[min_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').min()\nmin_prev.columns = ['min_' + f_ for f_ in min_prev.columns]\n\n#var_feats = ['DAYS_DECISION','CNT_PAYMENT','AMT_ANNUITY','AMT_CREDIT']\n#print('var_feats',var_feats)\n#var_prev = prev[var_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').var()\n#var_prev.columns = ['var_' + f_ for f_ in var_prev.columns]\n\n#exclude id, days, ratio for sum\nnosum_feats = ['SK_ID_CURR','SK_ID_PREV','DAYS_TOTAL','DAYS_TOTAL2','DAYS_FIRST_DRAWING',\n               'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION'\n               'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED',\n               'AMT_CREDIT_GOODS_PERC','APP_CREDIT_PERC']\nsum_feats = [f_ for f_ in prev.columns.values if not f_ in nosum_feats]\nprint ('sum_feats',sum_feats)\nsum_prev = prev[sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\nsum_prev.columns = ['sum_' + f_ for f_ in sum_prev.columns]\n\n\nfor cat_ in prev_cat_features:\n    print('prev_'+cat_+'_mode')\n    cols = [f_ for f_ in sum_prev.columns.values if f_.find(cat_)>=0]\n    sum_prev[cat_+'_mode'] = sum_prev[cols].idxmax(axis=1)\n    sum_prev[cat_+'_mode'], indexer = pd.factorize(sum_prev[cat_+'_mode'])\n    meanenc_feats.append('prev_'+cat_+'_mode')\n    if len(cols)>=10:\n        for col in cols:\n            del sum_prev[col]\n\nprev_active = prev.loc[(prev['DAYS_LAST_DUE'].isnull()) & (prev['DAYS_LAST_DUE_1ST_VERSION']>0)]\nprev_active['AMT_LEFT'] = prev_active['AMT_ANNUITY'] * prev_active['DAYS_LAST_DUE_1ST_VERSION']/365.25\nprev_active['AMT_PAID'] = prev_active['SK_ID_PREV'].map(inst_prev_last)\nprev_active['AMT_OWE'] = (prev_active['AMT_CREDIT'] - prev_active['AMT_DOWN_PAYMENT'].fillna(0)) * (1+prev_active['RATE_INTEREST_PRIVILEGED'].fillna(0))\nprev_active['AMT_LEFT2'] = prev_active['AMT_OWE']  - prev_active['AMT_PAID']\nprev_active['LEFT_RATIO'] = prev_active['SK_ID_PREV'].map(pos_prev_last['INSTAL_LEFT_RATIO'])\nprev_active['AMT_LEFT3'] = prev_active['AMT_CREDIT'] * prev_active['LEFT_RATIO']\nprev_active['AMT_PAY_YEAR_LEFT'] = prev_active['AMT_LEFT'] / prev_active['AMT_ANNUITY']\n\nactive_sum_feats = [f_ for f_ in prev_active.columns.values if (f_.find('AMT')>=0)]\nprint ('active_sum_feats',active_sum_feats)\nactive_sum_prev = prev_active[active_sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\nactive_sum_prev.columns = ['active_sum_' + f_ for f_ in active_sum_prev.columns]\nactive_sum_prev['active_count'] = prev_active[['SK_ID_PREV','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_PREV']\n\nactive_mean_feats = active_sum_feats\nprint ('active_mean_feats',active_mean_feats)\nactive_mean_prev = prev_active[active_mean_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').mean()\nactive_mean_prev.columns = ['active_mean_' + f_ for f_ in active_mean_prev.columns]\n\nnum_aggregations = {\n    'SK_ID_PREV': ['count'],\n    'AMT_ANNUITY': [ 'sum', 'median'],\n    'AMT_APPLICATION': [ 'max','median'],\n    'AMT_CREDIT': [ 'median', 'sum'],\n    'APP_CREDIT_PERC': [ 'max', 'mean'],\n    'AMT_DIFF_CREAPP': [ 'sum', 'median'],\n    'AMT_DIFF_CREDIT_GOODS': [ 'sum', 'median'],\n    'AMT_CREDIT_GOODS_PERC': [ 'max', 'median'],\n    'AMT_PAY_YEAR': [ 'sum', 'median'],\n    'AMT_DOWN_PAYMENT': [ 'sum', 'median'],\n    'RATE_DOWN_PAYMENT': [ 'max', 'mean'],\n    'DAYS_DECISION': [ 'max', 'mean', 'min'],\n    'CNT_PAYMENT': ['median', 'sum'],\n}\nprint('aggragate with in approved and refused applications')\napproved_prev = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1].groupby('SK_ID_CURR').agg(num_aggregations)\napproved_prev.columns = pd.Index(['approved_' + e[0] + \"_\" + e[1].upper() for e in approved_prev.columns.tolist()])\nrefused_prev = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1].groupby('SK_ID_CURR').agg(num_aggregations)\nrefused_prev.columns = pd.Index(['refused_' + e[0] + \"_\" + e[1].upper() for e in refused_prev.columns.tolist()])\n\nprint('aggragate with in defaulted and good applications')\ndefaulted_prev = prev[prev['DEFAULTED']==1].groupby('SK_ID_CURR').agg(num_aggregations)\ndefaulted_prev.columns = pd.Index(['defaulted_' + e[0] + \"_\" + e[1].upper() for e in defaulted_prev.columns.tolist()])\ngood_prev = prev[(prev['DEFAULTED']==0) & (prev['NAME_CONTRACT_STATUS_Approved'] == 1)].groupby('SK_ID_CURR').agg(num_aggregations)\ngood_prev.columns = pd.Index(['good_' + e[0] + \"_\" + e[1].upper() for e in good_prev.columns.tolist()])\n\nprint('find one with closest annuity/credit')\ntmp = pd.concat([data[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY']], test[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY']]], axis=0).set_index('SK_ID_CURR')\nprev['AMT_CREDIT_DIFF'] = (prev['AMT_CREDIT'] - prev['SK_ID_CURR'].map(tmp['AMT_CREDIT'])).abs()\nprev['AMT_ANNUITY_DIFF'] = (prev['AMT_ANNUITY'] - prev['SK_ID_CURR'].map(tmp['AMT_ANNUITY'])).abs()\n\nidx = prev.groupby('SK_ID_CURR')['AMT_CREDIT_DIFF'].idxmin()\nidx = idx.loc[~idx.isnull()]\nprev_closest_credit_defaulted = prev[['SK_ID_CURR','DEFAULTED']].loc[idx].set_index('SK_ID_CURR')\nprev_closest_credit_defaulted.rename({'DEFAULTED':'closest_credit_defaulted'},axis=1,inplace=True)\n\nidx = prev.groupby('SK_ID_CURR')['AMT_ANNUITY_DIFF'].idxmin()\nidx = idx.loc[~idx.isnull()]\nprev_closest_annuity_defaulted = prev[['SK_ID_CURR','DEFAULTED']].loc[idx].set_index('SK_ID_CURR')\nprev_closest_annuity_defaulted.rename({'DEFAULTED':'closest_annuity_defaulted'},axis=1,inplace=True)\n\n#merge...\nprint('merge')\navg_prev = avg_prev.merge(max_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(sum_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(min_prev, on='SK_ID_CURR', how='outer')\n#avg_prev = avg_prev.merge(var_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(active_sum_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(active_mean_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(approved_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(refused_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(defaulted_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(good_prev, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(prev_recent, on='SK_ID_CURR', how='outer')\n#avg_prev = avg_prev.merge(prev_last_refused, on='SK_ID_CURR', how='outer')\n#avg_prev = avg_prev.merge(prev_last_approved, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(prev_closest_credit_defaulted, on='SK_ID_CURR', how='outer')\navg_prev = avg_prev.merge(prev_closest_annuity_defaulted, on='SK_ID_CURR', how='outer')\navg_prev['count'] = prev[['SK_ID_PREV','SK_ID_CURR']].groupby('SK_ID_CURR')['SK_ID_PREV'].count()\navg_prev['DEFALUTED_RATIO'] = prev[['SK_ID_CURR','DEFAULTED']].groupby('SK_ID_CURR')['DEFAULTED'].mean()\n#del avg_buro['SK_ID_PREV']\n\navg_prev.columns = ['prev_' + f_ for f_ in avg_prev.columns]\n\n#avg_prev.fillna(0,inplace=True)\navg_prev = downcast_dtypes(avg_prev)\ndel prev, prev_recent, sum_prev, active_sum_prev\ndel approved_prev, refused_prev\n#del defaulted_prev, good_prev\ngc.collect()\navg_prev.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec9ce25e1c2a831da9729e7138c433514543a4c6","collapsed":true},"cell_type":"code","source":"avg_prev.to_csv('avg_prev.csv', compression='zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72b82e8215c11c6294833e28771286b51b7d111f","collapsed":true},"cell_type":"code","source":"# Now merge all the data\ndata = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\ndata = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\ndata = data.merge(right=ccbl_mon.reset_index(), how='left', on='SK_ID_CURR')\n#data = data.merge(right=ccbl_fit.reset_index(), how='left', on='SK_ID_CURR')\ndata = data.merge(right=pos_recent.reset_index(), how='left', on='SK_ID_CURR')\ndata = data.merge(right=inst_avg.reset_index(), how='left', on='SK_ID_CURR')\n\ntest = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=ccbl_mon.reset_index(), how='left', on='SK_ID_CURR')\n#test = test.merge(right=ccbl_fit.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=pos_recent.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=inst_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n#data.fillna(-1,inplace=True)\n#test.fillna(-1,inplace=True)\n\ndel avg_prev, avg_buro, ccbl_mon, pos_recent, inst_avg\n#del ccbl_fit\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a32a2bf90ed9b3f70ba054ac470f672b4c7cd9ac"},"cell_type":"code","source":"#cols = ['SK_ID_CURR','cc_month_score_max','cc_month_score_sum','cc_month_score_mean','cc_month_score_std']\n#agg_cc_month_score = pd.read_csv('../input/credit-risk-month-training-aggragation/agg_cc_month_score.csv',compression='zip', usecols=cols)\n#data = data.merge(right=agg_cc_month_score, how='left', on='SK_ID_CURR')\n#test = test.merge(right=agg_cc_month_score, how='left', on='SK_ID_CURR')\n#del agg_cc_month_score\n#gc.collect()\n\n#cols = ['SK_ID_CURR','prev_month_score_max','prev_month_score_sum','prev_month_score_mean','prev_month_score_std']\n#agg_prev_month_score = pd.read_csv('../input/credit-risk-month-training-aggragation/agg_prev_month_score.csv',compression='zip', usecols=cols)\n#data = data.merge(right=agg_prev_month_score, how='left', on='SK_ID_CURR')\n#test = test.merge(right=agg_prev_month_score, how='left', on='SK_ID_CURR')\n#del agg_prev_month_score\n#gc.collect()\n\ncols = ['SK_ID_CURR','month_score_max','month_score_std','month_score_mean','month_score_sub2_sum','month_score_top3','month_score_recent4y_sum']\nagg_month_score = pd.read_csv('../input/credit-risk-month-training-aggragation/agg_month_score2.csv',usecols=cols,compression='zip')\ndata = data.merge(right=agg_month_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=agg_month_score, how='left', on='SK_ID_CURR')\ndel agg_month_score\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f9e9ab6f60d2369062088d604326eb93bc2ab78","collapsed":true},"cell_type":"code","source":"agg_prev_score = pd.read_csv('../input/credit-risk-prev-buro-training-agg/agg_prev_score2.csv', compression='zip')\nagg_buro_score = pd.read_csv('../input/credit-risk-prev-buro-training-agg/agg_buro_score2.csv', compression='zip')\nagg_prev_score.drop(['TARGET','prev_score_sub1','prev_score_sub3','prev_score_sum','prev_score_recent3y_sum'],axis=1,inplace=True)\nagg_buro_score.drop(['TARGET','buro_score_sub1','buro_score_sub3','buro_score_sum','buro_score_recent3y_sum'],axis=1,inplace=True)\ndata = data.merge(right=agg_prev_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=agg_prev_score, how='left', on='SK_ID_CURR')\ndata = data.merge(right=agg_buro_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=agg_buro_score, how='left', on='SK_ID_CURR')\ndel agg_prev_score, agg_buro_score\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ab98bb2a0981635456c92bf2a0dd3a99d2ba73","collapsed":true},"cell_type":"code","source":"prev_prev_score = pd.read_csv('../input/credit-risk-prev-aug/prev_prev_score1.csv')\ndata = data.merge(right=prev_prev_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=prev_prev_score, how='left', on='SK_ID_CURR')\n\nprev_prev_score2 = pd.read_csv('../input/credit-risk-prev-aug-2/prev_prev_score2.csv')\ndata = data.merge(right=prev_prev_score2, how='left', on='SK_ID_CURR')\ntest = test.merge(right=prev_prev_score2, how='left', on='SK_ID_CURR')\n\ndata.groupby('TARGET')[['prev_prev_score1','prev_prev_score2']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6668513f5bd81537bcb17e6964f6c42c97605498","collapsed":true},"cell_type":"code","source":"train_house_score = pd.read_csv('../input/house-features/train_house_score.csv')\ntest_house_score = pd.read_csv('../input/house-features/test_house_score.csv')\nhouse_score_ext = pd.read_csv('../input/house-features/house_ex.csv')\n\ndata = data.merge(right=train_house_score, how='left', on='SK_ID_CURR')\ndata = data.merge(right=house_score_ext, how='left', on='SK_ID_CURR')\n\ntest = test.merge(right=test_house_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=house_score_ext, how='left', on='SK_ID_CURR')\n\ntrain_cc_score = pd.read_csv('../input/credit-card-balance-lstm/cc_score_train.csv')\ntest_cc_score = pd.read_csv('../input/credit-card-balance-lstm/cc_score_test.csv')\ndata = data.merge(right=train_cc_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=test_cc_score, how='left', on='SK_ID_CURR')\n\ntrain_bubl_score = pd.read_csv('../input/buro-lstm/bubl_score_train.csv')\ntest_bubl_score = pd.read_csv('../input/buro-lstm/bubl_score_test.csv')\ndata = data.merge(right=train_bubl_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=test_bubl_score, how='left', on='SK_ID_CURR')\n\ntrain_pos_score = pd.read_csv('../input/pos-lstm/pos_score_train.csv')\ntest_pos_score = pd.read_csv('../input/pos-lstm/pos_score_test.csv')\ndata = data.merge(right=train_pos_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=test_pos_score, how='left', on='SK_ID_CURR')\n\ntrain_inst_score = pd.read_csv('../input/inst-lstm/inst_score_train.csv')\ntest_inst_score = pd.read_csv('../input/inst-lstm/inst_score_test.csv')\ndata = data.merge(right=train_inst_score, how='left', on='SK_ID_CURR')\ntest = test.merge(right=test_inst_score, how='left', on='SK_ID_CURR')\n\n#prev_score = pd.read_csv('../input/credit-risk-prev/prev_score.csv')\n#prev_score.rename({'TARGET':'prev_score'},axis=1,inplace=True)\n#data = data.merge(right=prev_score.iloc[:train_size,:], how='left', on='SK_ID_CURR')\n#test = test.merge(right=prev_score.iloc[-test_size:,:], how='left', on='SK_ID_CURR')\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17189a223fa848323ef281030c3ca40e63b70c0a","collapsed":true},"cell_type":"code","source":"#more feature after merge\ndata['Total_AMT_ANNUITY'] = data[['AMT_ANNUITY','bureau_active_sum_AMT_ANNUITY','prev_active_sum_AMT_ANNUITY']].sum(axis=1)\ndata['Total_ANNUITY_INCOME_RATIO'] = data['Total_AMT_ANNUITY'] / data['AMT_INCOME_TOTAL']\ndata['Total_CREDIT'] = data[['AMT_CREDIT','prev_active_sum_AMT_LEFT']].sum(axis=1) #exclude AMT already paid\ndata['Total_CREDIT_INCOME_RATIO'] = data['Total_CREDIT'] / data['AMT_INCOME_TOTAL']\ndata['Total_acc'] = data[['prev_count','bureau_count']].sum(axis=1)\ndata['Total_active_acc'] = data[['prev_active_count','bureau_active_count']].sum(axis=1)\ndata['Total_AMT_LEFT'] = data['AMT_CREDIT'] + data['prev_active_sum_AMT_LEFT'] + data['bureau_active_sum_AMT_CREDIT_LEFT']\ndata['Total_AMT_LEFT_INCOME_RATIO'] = data['Total_AMT_LEFT']/data['AMT_INCOME_TOTAL']\n\ntest['Total_AMT_ANNUITY'] = test[['AMT_ANNUITY','bureau_active_sum_AMT_ANNUITY','prev_active_sum_AMT_ANNUITY']].sum(axis=1)\ntest['Total_ANNUITY_INCOME_RATIO'] = test['Total_AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\ntest['Total_CREDIT'] = test[['AMT_CREDIT','prev_active_sum_AMT_LEFT']].sum(axis=1)\ntest['Total_CREDIT_INCOME_RATIO'] = test['Total_CREDIT'] / test['AMT_INCOME_TOTAL']\ntest['Total_acc'] = test[['prev_count','bureau_count']].sum(axis=1)\ntest['Total_active_acc'] = test[['prev_active_count','bureau_active_count']].sum(axis=1)\ntest['Total_AMT_LEFT'] = test['AMT_CREDIT'] + test['prev_active_sum_AMT_LEFT'] + test['bureau_active_sum_AMT_CREDIT_LEFT']\ntest['Total_AMT_LEFT_INCOME_RATIO'] = test['Total_AMT_LEFT']/test['AMT_INCOME_TOTAL']\n\nshared_feats = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_PAY_YEAR', \n                'AMT_DIFF_CREDIT_GOODS', 'AMT_CREDIT_GOODS_PERC']\nfor f_ in shared_feats:\n    data[f_+'_to_prev_approved'] = (data[f_] - data['prev_approved_'+f_+'_MEDIAN'])/data['prev_approved_'+f_+'_MEDIAN']\n    data[f_+'_to_prev_refused'] = (data[f_] - data['prev_refused_'+f_+'_MEDIAN'])/data['prev_refused_'+f_+'_MEDIAN']\n    #data[f_+'_to_prev_defaulted'] = (data[f_] - data['prev_defaulted_'+f_+'_MEAN'])/data['prev_defaulted_'+f_+'_MEAN']\n    #data[f_+'_to_prev_good'] = (data[f_] - data['prev_good_'+f_+'_MEAN'])/data['prev_good_'+f_+'_MEAN']\n    test[f_+'_to_prev_approved'] = (test[f_] - test['prev_approved_'+f_+'_MEDIAN'])/test['prev_approved_'+f_+'_MEDIAN']\n    test[f_+'_to_prev_refused'] = (test[f_] - test['prev_refused_'+f_+'_MEDIAN'])/test['prev_refused_'+f_+'_MEDIAN']\n    #test[f_+'_to_prev_defaulted'] = (test[f_] - test['prev_defaulted_'+f_+'_MEAN'])/test['prev_defaulted_'+f_+'_MEAN']\n    #test[f_+'_to_prev_good'] = (test[f_] - test['prev_good_'+f_+'_MEAN'])/test['prev_good_'+f_+'_MEAN']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce4cb098166bbed0775f10f23480c2b1985b7b5","collapsed":true},"cell_type":"code","source":"print('num of features:',data.shape[1])\nprint('meanenc_feats')\nmeanenc_feats = list(set(meanenc_feats))\nprint(len(meanenc_feats), meanenc_feats)\nprint('categorical_feats')\nprint(len(cat_feats), cat_feats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb6ba885fc541c198bbd328dd62576335b258953"},"cell_type":"markdown","source":"y = data['TARGET']\ncorr = pd.Series()\nfor f_ in data.columns:\n    if not data[f_].dtype is object:\n        corr[f_] = y.corr(data[f_])\n    \nprint(corr.apply(np.abs).sort_values())"},{"metadata":{"trusted":true,"_uuid":"1d2ff0f89a1824354f58dcc6a599e9d7e0769614","collapsed":true},"cell_type":"markdown","source":"#y = data['TARGET']\n#corr = pd.Series()\n#for f_ in data.columns:\n    #if not data[f_].dtype is object:\n        #corr[f_] = y.corr(data[f_])\n    \n#print(corr.apply(np.abs).sort_values())\ncorr_mat = data.iloc[:,2:].corr().abs()\nthresh = 0.95\ncolinear_feats = []\nfor i in range(corr_mat.shape[1]):\n    for j in range (0,i):\n        if corr_mat.iloc[i,j] > thresh:\n            feat1 = corr_mat.columns.values[i]\n            feat2 = corr_mat.index.values[j]\n            print('correlation between ', feat1, feat2, ' is: ', corr_mat.iloc[i,j])\n            if not feat1 in colinear_feats:\n                colinear_feats.append(feat1)\nprint (colinear_feats)"},{"metadata":{"trusted":true,"_uuid":"a819267a1ab8fce77b2806f81f0cdef7e261aff7","collapsed":true},"cell_type":"code","source":"# Serious training....\n# Get features\nexcluded_feats = ['SK_ID_CURR','TARGET'] + ['prev_sum_CODE_REJECT_REASON_CLIENT','bureau_sum_CREDIT_ACTIVE_Active']# + list(feature_importance.feature.values[-50:])\nprint(excluded_feats)\n#sample_feats(excluded_feats, feature_importance, percent=0.38, baseprob=0.1)\n\n#create hold out\n#data, hdt = train_test_split(data, test_size=0.04, shuffle=True, stratify=data['TARGET']) \n#hdt_y = hdt['TARGET']\ny = data['TARGET']\n\n# Run a 5 fold\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=90210)\noof_preds = np.zeros(data.shape[0])\n#hdt_preds = np.zeros(hdt.shape[0])\nsub_preds = np.zeros(test.shape[0])\nfeature_importance_df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"453db7c837e2445e355069967347d1cb3a6e9300","collapsed":true},"cell_type":"code","source":"from copy import deepcopy\n\nclass bagging_classifier:\n\n    def __init__(self, base_estimator, n_estimators):\n\n        self.base_estimator_ = base_estimator\n        self.n_estimators_ = n_estimators\n\n    def fit(self, X, y, eval_set = None, eval_metric = None, verbose = None, early_stopping_rounds = None, categorical_feature = None):\n        \n        self.estimators_ = []\n        self.feature_importances_gain_ = np.zeros(X.shape[1])\n        self.feature_importances_split_ = np.zeros(X.shape[1])\n        self.n_classes_ = y.nunique()\n\n        if self.n_estimators_ == 1:\n            print ('n_estimators=1, no downsampling')\n            estimator = deepcopy(self.base_estimator_)\n            estimator.fit(X, y, eval_set = [(X, y)] + eval_set,\n                eval_metric = eval_metric, verbose = verbose, \n                early_stopping_rounds = early_stopping_rounds)\n            self.estimators_.append(estimator)\n            self.feature_importances_gain_ += estimator.booster_feature_importance(importance_type='gain')\n            self.feature_importances_split_ += estimator.booster_feature_importance(importance_type='split')\n            return\n\n    #average down sampling results\n        minority = y.value_counts().sort_values().index.values[0]\n        majority = y.value_counts().sort_values().index.values[1]\n        print('majority class:', majority)\n        print('minority class:', minority)\n\n        X_min = X.loc[y==minority]\n        y_min = y.loc[y==minority]\n        X_maj = X.loc[y==majority]\n        y_maj = y.loc[y==majority]\n\n        kf = KFold(self.n_estimators_, shuffle=True, random_state=42)\n\n        for rest, this in kf.split(y_maj):\n\n            print('training on a subset')\n            X_maj_sub = X_maj.iloc[this]\n            y_maj_sub = y_maj.iloc[this]\n            X_sub = pd.concat([X_min, X_maj_sub])\n            y_sub = pd.concat([y_min, y_maj_sub])\n\n            estimator = deepcopy(self.base_estimator_)\n\n            estimator.fit(X_sub, y_sub, eval_set = [(X_sub, y_sub)] + eval_set,\n                eval_metric = eval_metric, verbose = verbose, \n                early_stopping_rounds = early_stopping_rounds,\n                categorical_feature = categorical_feature)\n\n            self.estimators_.append(estimator)\n            self.feature_importances_gain_ += estimator.booster_.feature_importance(importance_type='gain')/self.n_estimators_\n            self.feature_importances_split_ += estimator.booster_.feature_importance(importance_type='split')/self.n_estimators_\n\n\n    def predict_proba(self, X):\n\n        n_samples = X.shape[0]\n        proba = np.zeros([n_samples, self.n_classes_])\n\n        for estimator in self.estimators_:\n\n            proba += estimator.predict_proba(X, num_iteration=estimator.best_iteration_)/self.n_estimators_\n\n        return proba\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b29c6a0d02a2477d7b06e28928c49cdf53ce7a22","trusted":true,"collapsed":true},"cell_type":"code","source":"scores = []\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(data,data['TARGET'])):\n    trn, val = data.iloc[trn_idx], data.iloc[val_idx]\n\n#smote\n#    features = [f_ for f_ in trn.columns if f_ not in excluded_feats]\n#    X = trn[features]\n#    Y = trn['TARGET']\n#    print(Y.value_counts())\n#    nsample_dict={0.0:Y.value_counts().loc[0.0],1.0:Y.value_counts().loc[1.0]*3}\n#    X,Y = smote_nc(X,Y,meanenc_feats,k_neighbors=2,nsample_dict=nsample_dict)    \n#    trn = pd.concat([X,Y],axis=1)\n#    print(Y.value_counts())\n\n#combine val and test to be encoded\n    #val_test = pd.concat([val,hdt,test],axis=0,sort=False)\n    val_test = pd.concat([val,test],axis=0,sort=False)\n    val_size = val.shape[0]\n    #hdt_size = hdt.shape[0]\n    test_size = test.shape[0]\n    print ('doing mean_encoding')\n    trn, val_test = mean_encode(trn, val_test, meanenc_feats, 'TARGET', drop=True)\n    features = [f_ for f_ in trn.columns if f_ not in excluded_feats]\n    \n    val  = val_test.iloc[0:val_size, :].copy(deep=True)\n    #hdt_x = val_test[features].iloc[val_size:val_size+hdt_size, :].copy(deep=True)\n    #test_x = test[features] #if not doing mean encoding\n    test_x = val_test[features].iloc[-test_size:,:].copy(deep=True)\n        \n    trn_x, trn_y = trn[features], trn['TARGET']\n    val_x, val_y = val[features], val['TARGET']\n    print('done mean_encoding')\n    \n#    clf = LGBMClassifier(\n    model = LGBMClassifier(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=26,\n        colsample_bytree=0.28,\n        subsample=0.95,\n        max_depth=4,\n        reg_alpha=4.8299,\n        reg_lambda=3.6335,\n        min_split_gain=0.005,\n        min_child_weight=40,\n        silent=True,\n        verbose=-1,\n        n_jobs = 16,\n        random_state = n_fold * 6666,\n        class_weight = {0:1,1:1}\n    )\n    \n    clf = bagging_classifier(model, 3)\n\n    clf.fit(trn_x, trn_y, \n            eval_set= [(val_x, val_y)], \n            eval_metric='auc', verbose=100, early_stopping_rounds=100,\n            categorical_feature = cat_feats,\n           )\n    \n    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n    sub_preds += clf.predict_proba(test_x)[:, 1] / folds.n_splits\n    #hdt_subpred = clf.predict_proba(hdt_x)[:, 1]\n    #hdt_preds += hdt_subpred / folds.n_splits\n    \n    fold_score = roc_auc_score(val_y, oof_preds[val_idx])\n    scores.append(fold_score)\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, fold_score))\n    #print('Fold %2d holdout AUC : %.6f' % (n_fold + 1, roc_auc_score(hdt_y, hdt_subpred)))\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance_gain\"] = clf.feature_importances_gain_\n    fold_importance_df[\"importance_split\"] = clf.feature_importances_split_\n    fold_importance_df[\"fold\"] = n_fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    del clf, trn_x, trn_y, val_x, val_y\n    del trn, val\n    gc.collect()\n    \nprint('Full AUC score %.6f +- %0.4f' % (roc_auc_score(y, oof_preds), np.std(scores)))\n#print('Full holdout AUC score %.6f' % roc_auc_score(hdt_y, hdt_preds)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9454b1017c76709029ade36676ec841166b0aa","collapsed":true},"cell_type":"code","source":"prob_df = pd.DataFrame({'SK_ID_CURR': data['SK_ID_CURR'], 'prob': oof_preds, 'target': data['TARGET']})\n#prob_df.head()\nsns.distplot(prob_df['prob'].loc[prob_df.target==1] , color=\"skyblue\", label=\"target 1\")\nsns.distplot(prob_df['prob'].loc[prob_df.target==0] , color=\"red\", label=\"target 0\")\nplt.legend()\nprob_df.to_csv('prob_oof_pred',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"193b29cda910cbc9f413062ee7f85210494d14c3","trusted":true,"collapsed":true},"cell_type":"code","source":"test['TARGET'] = sub_preds\n\ntest[['SK_ID_CURR', 'TARGET']].to_csv('first_submission.csv', index=False, float_format='%.8f')\n\n# Plot feature importances\nfeature_importance = feature_importance_df[[\"feature\", \"importance_gain\", \"importance_split\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance_split\", ascending=False)\n#feature_importance['correlation'] = corr.loc[feature_importance.index.values]\nfeature_importance.to_csv('feature_importance.csv')\n\nbest_features = feature_importance.iloc[:50].reset_index()\n\nimport matplotlib.gridspec as gridspec\nfig = plt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='importance_split', y='feature', data=best_features, ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='importance_gain', y='feature', data=best_features, ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\nfig.subplots_adjust(top=0.93)\nplt.savefig('lgbm_importances.png')\n\n# Plot ROC curves\nplt.figure(figsize=(6,6))\nscores = [] \nfor n_fold, (_, val_idx) in enumerate(folds.split(data, data['TARGET'])):  \n    # Plot the roc curve\n    fpr, tpr, thresholds = roc_curve(y.iloc[val_idx], oof_preds[val_idx])\n    score = roc_auc_score(y.iloc[val_idx], oof_preds[val_idx])\n    scores.append(score)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\nfpr, tpr, thresholds = roc_curve(y, oof_preds)\nscore = roc_auc_score(y, oof_preds)\nplt.plot(fpr, tpr, color='b',\n         label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n         lw=2, alpha=.8)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('LightGBM ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\n\nplt.savefig('roc_curve.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}