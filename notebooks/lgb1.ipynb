{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM classifier 1\n",
    "\n",
    "* Single model for home credit default risk.\n",
    "* Create features from main application table.\n",
    "* Create features from supplementary tables (prev/bureau/installment/pos-cash/credit card), groupy by current ID and merge to main table.\n",
    "* Read preprocessed features generated from other notebooks and merge to main table.\n",
    "* Fit LGBMClassifier and save results to disk for ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "    \n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility functions\n",
    "* downcast_type to save memory space\n",
    "* mean_encoding for categorical features. Use KFold regularization to avoid leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27eb9cc93c1b498c6b451389a336b3837b34ba2b"
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype in [\"int64\"]]\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "\n",
    "    return df\n",
    "\n",
    "def mean_encode(train, val, features_to_encode, target, drop=False):\n",
    "    train_encode = train.copy(deep=True)\n",
    "    val_encode = val.copy(deep=True)\n",
    "    for feature in features_to_encode:\n",
    "        train_global_mean = train[target].mean()\n",
    "        train_encode_map = pd.DataFrame(index = train[feature].unique())\n",
    "        train_encode[feature+'_mean_encode'] = np.nan\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "        for rest, this in kf.split(train):\n",
    "            train_rest_global_mean = train[target].iloc[rest].mean()\n",
    "            encode_map = train.iloc[rest].groupby(feature)[target].mean()\n",
    "            encoded_feature = train.iloc[this][feature].map(encode_map).values\n",
    "            train_encode[feature+'_mean_encode'].iloc[this] = train[feature].iloc[this].map(encode_map).values\n",
    "            train_encode_map = pd.concat((train_encode_map, encode_map), axis=1, sort=False)\n",
    "            train_encode_map.fillna(train_rest_global_mean, inplace=True) \n",
    "            train_encode[feature+'_mean_encode'].fillna(train_rest_global_mean, inplace=True)\n",
    "            \n",
    "        train_encode_map['avg'] = train_encode_map.mean(axis=1)\n",
    "        val_encode[feature+'_mean_encode'] = val[feature].map(train_encode_map['avg'])\n",
    "        val_encode[feature+'_mean_encode'].fillna(train_global_mean,inplace=True)\n",
    "        \n",
    "    if drop: #drop unencoded features\n",
    "        train_encode.drop(features_to_encode, axis=1, inplace=True)\n",
    "        val_encode.drop(features_to_encode, axis=1, inplace=True)\n",
    "    return train_encode, val_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main application table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa9b32922310be8c282b77cefa61dbd50c8bf5a5"
   },
   "outputs": [],
   "source": [
    "meanenc_feats = []\n",
    "cat_feats = []\n",
    "\n",
    "data = pd.read_csv('../input/application_train.csv.zip')\n",
    "test = pd.read_csv('../input/application_test.csv.zip')\n",
    "y = data['TARGET']\n",
    "#####process train and test together######\n",
    "combined = data.append(test,sort=False)\n",
    "train_size = data.shape[0]\n",
    "test_size = test.shape[0]\n",
    "\n",
    "combined['CODE_GENDER'].replace('XNA', np.nan, inplace=True)\n",
    "combined['NAME_FAMILY_STATUS'].replace('Unknown', np.nan, inplace=True)\n",
    "combined['ORGANIZATION_TYPE'].replace('XNA', np.nan, inplace=True)\n",
    "combined['DAYS_EMPLOYED'].loc[combined['DAYS_EMPLOYED']==365243] = np.nan\n",
    "#create some new features\n",
    "#from https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features/code\n",
    "docs = [f_ for f_ in combined.columns if 'FLAG_DOC' in f_]\n",
    "live = [f_ for f_ in combined.columns if ('FLAG_' in f_) & ('FLAG_DOC' not in f_) & ('_FLAG_' not in f_)]\n",
    "combined['NEW_DOC_IND_KURT'] = combined[docs].kurtosis(axis=1)\n",
    "combined['NEW_LIVE_IND_SUM'] = combined[live].sum(axis=1)\n",
    "combined['NEW_INC_PER_CHLD'] = combined['AMT_INCOME_TOTAL'] / (1 + combined['CNT_CHILDREN'])\n",
    "inc_by_org = combined[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "combined['NEW_INC_BY_ORG'] = combined['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "combined['NEW_EMPLOY_TO_BIRTH_RATIO'] = combined['DAYS_EMPLOYED'] / combined['DAYS_BIRTH']\n",
    "combined['NEW_SOURCES_PROD'] = combined['EXT_SOURCE_1'] * combined['EXT_SOURCE_2'] * combined['EXT_SOURCE_3']\n",
    "combined['NEW_EXT_SOURCES_MEAN'] = combined[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "combined['NEW_SCORES_STD'] = combined[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "combined['NEW_SCORES_STD'] = combined['NEW_SCORES_STD'].fillna(combined['NEW_SCORES_STD'].mean())\n",
    "combined['NEW_CAR_TO_BIRTH_RATIO'] = combined['OWN_CAR_AGE'] / combined['DAYS_BIRTH']\n",
    "combined['NEW_CAR_TO_EMPLOY_RATIO'] = combined['OWN_CAR_AGE'] / combined['DAYS_EMPLOYED']\n",
    "combined['NEW_PHONE_TO_BIRTH_RATIO'] = combined['DAYS_LAST_PHONE_CHANGE'] / combined['DAYS_BIRTH']\n",
    "combined['NEW_PHONE_TO_EMPLOYED_RATIO'] = combined['DAYS_LAST_PHONE_CHANGE'] / combined['DAYS_EMPLOYED']\n",
    "combined['NEW_CREDIT_TO_INCOME_RATIO'] = combined['AMT_CREDIT'] / combined['AMT_INCOME_TOTAL']\n",
    "\n",
    "combined['AMT_PAY_YEAR'] = combined['AMT_CREDIT']/combined['AMT_ANNUITY']\n",
    "combined['AGE_PAYOFF'] = -combined['DAYS_BIRTH']/365.25 + combined['AMT_PAY_YEAR']\n",
    "combined['AMT_ANNUITY_INCOME_RATE'] = combined['AMT_ANNUITY']/combined['AMT_INCOME_TOTAL']\n",
    "combined['AMT_DIFF_CREDIT_GOODS'] = combined['AMT_CREDIT'] - combined['AMT_GOODS_PRICE']\n",
    "combined['AMT_CREDIT_GOODS_PERC'] = combined['AMT_CREDIT'] / combined['AMT_GOODS_PRICE']\n",
    "combined['DOCUMENT_CNT'] = combined.loc[:,combined.columns.str.startswith('FLAG_DOCUMENT')].sum(axis=1) #not sure about this\n",
    "combined['AGE_EMPLOYED'] = combined['DAYS_EMPLOYED'] - combined['DAYS_BIRTH']\n",
    "combined['AMT_INCOME_OVER_CHILD'] = combined['AMT_INCOME_TOTAL']/combined['CNT_CHILDREN']\n",
    "combined['CNT_ADULT'] = combined['CNT_FAM_MEMBERS']-combined['CNT_CHILDREN']\n",
    "combined['ADULT_RATIO'] = combined['CNT_ADULT']/combined['CNT_FAM_MEMBERS']\n",
    "combined['AMT_REQ_CREDIT_BUREAU_MON_CHANGE'] = combined['AMT_REQ_CREDIT_BUREAU_QRT']/2 - combined['AMT_REQ_CREDIT_BUREAU_MON']\n",
    "combined['AMT_REQ_CREDIT_BUREAU_QRT_CHANGE'] = combined['AMT_REQ_CREDIT_BUREAU_YEAR']/3 - combined['AMT_REQ_CREDIT_BUREAU_QRT']\n",
    "combined['AMT_REQ_CREDIT_BUREAU_TOTAL'] = combined['AMT_REQ_CREDIT_BUREAU_HOUR'] + combined['AMT_REQ_CREDIT_BUREAU_DAY'] \n",
    "+ combined['AMT_REQ_CREDIT_BUREAU_MON'] + combined['AMT_REQ_CREDIT_BUREAU_QRT'] + combined['AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "\n",
    "combined['REGION'], indexer = pd.factorize(combined['REGION_POPULATION_RELATIVE'])\n",
    "meanenc_feats.append('REGION')\n",
    "\n",
    "combined['GENDER_FAMILY_STATUS'] = combined['CODE_GENDER'].astype('str') + combined['NAME_FAMILY_STATUS']\n",
    "combined['CNT_CHILDREN_CLIPPED'] = combined['CNT_CHILDREN'].clip(0,10)\n",
    "\n",
    "#how does clients income compare to ...\n",
    "gender_mean_income = combined.groupby('CODE_GENDER')['AMT_INCOME_TOTAL'].median()\n",
    "own_car_mean_income = combined.groupby('FLAG_OWN_CAR')['AMT_INCOME_TOTAL'].median()\n",
    "own_realty_mean_income = combined.groupby('FLAG_OWN_REALTY')['AMT_INCOME_TOTAL'].median()\n",
    "cnt_children_mean_income = combined.groupby('CNT_CHILDREN_CLIPPED')['AMT_INCOME_TOTAL'].median()\n",
    "region_mean_income = combined.groupby('REGION')['AMT_INCOME_TOTAL'].median()\n",
    "family_status_mean_income = combined.groupby('NAME_FAMILY_STATUS')['AMT_INCOME_TOTAL'].median()\n",
    "gender_family_status_mean_income = combined.groupby('GENDER_FAMILY_STATUS')['AMT_INCOME_TOTAL'].median()\n",
    "\n",
    "combined['gender_mean_income'] = combined['CODE_GENDER'].map(gender_mean_income)\n",
    "combined['own_car_mean_income'] = combined['FLAG_OWN_CAR'].map(own_car_mean_income)\n",
    "combined['own_realty_mean_income'] = combined['FLAG_OWN_REALTY'].map(own_realty_mean_income)\n",
    "combined['cnt_children_mean_income'] = combined['CNT_CHILDREN_CLIPPED'].map(cnt_children_mean_income)\n",
    "combined['region_mean_income'] = combined['REGION'].map(region_mean_income)\n",
    "combined['family_status_mean_income'] = combined['NAME_FAMILY_STATUS'].map(family_status_mean_income)\n",
    "combined['gender_family_status_mean_income'] = combined['GENDER_FAMILY_STATUS'].map(gender_family_status_mean_income)\n",
    "\n",
    "combined['gender_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['gender_mean_income'])/combined['gender_mean_income']\n",
    "combined['own_car_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['own_car_mean_income'])/combined['own_car_mean_income']\n",
    "combined['own_realty_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['own_realty_mean_income'])/combined['own_realty_mean_income']\n",
    "combined['cnt_children_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['cnt_children_mean_income'])/combined['cnt_children_mean_income']\n",
    "combined['region_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['region_mean_income'])/combined['region_mean_income']\n",
    "combined['family_status_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['family_status_mean_income'])/combined['family_status_mean_income']\n",
    "combined['gender_family_status_mean_income_rel'] = (combined['AMT_INCOME_TOTAL'] - combined['gender_family_status_mean_income'])/combined['gender_family_status_mean_income']\n",
    "\n",
    "#these features hightly correlated with others\n",
    "rejected_features = ['AMT_GOODS_PRICE',\n",
    "                     'APARTMENTS_AVG','APARTMENTS_MEDI',\n",
    "                     'BASEMENTAREA_AVG','BASEMENTAREA_MODE','COMMONAREA_AVG','COMMONAREA_MODE',\n",
    "                     'ELEVATORS_AVG','ELEVATORS_MEDI','ENTRANCES_AVG','ENTRANCES_MEDI','FLOORSMAX_AVG','FLOORSMAX_MEDI',\n",
    "                     'FLOORSMIN_AVG','FLOORSMIN_MEDI','LANDAREA_AVG','LANDAREA_MODE',\n",
    "                     'LIVINGAPARTMENTS_AVG','LIVINGAPARTMENTS_MEDI',\n",
    "                     'LIVINGAREA_AVG','LIVINGAREA_MODE',\n",
    "                     'NONLIVINGAPARTMENTS_AVG','NONLIVINGAPARTMENTS_MEDI',\n",
    "                     'NONLIVINGAREA_AVG','NONLIVINGAREA_MODE','OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "                     'REGION_RATING_CLIENT','YEARS_BEGINEXPLUATATION_AVG','YEARS_BEGINEXPLUATATION_MEDI',\n",
    "                     'YEARS_BUILD_AVG','YEARS_BUILD_MEDI']\n",
    "#lets see if we can exclude these..\n",
    "rejected_featues = rejected_features + ['ELEVATORS_MODE','ENTRANCE_MODE','FLOORSMAX_MEDI','FLOORSMIN_MEDI',\n",
    "                    'NONLIVINGAPARTMENTS_MODE',]#'LIVINGAPARTMENTS_MODE',\n",
    "#these features are not informative\n",
    "rejected_features = rejected_features + ['FLAG_MOBIL','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_2',\n",
    "                                        'WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START']\n",
    "rejected_features = rejected_features + ['gender_mean_income', 'own_car_mean_income', 'own_realty_mean_income', \n",
    "                                         'cnt_children_mean_income', 'family_status_mean_income',\n",
    "                                         'gender_family_status_mean_income',\n",
    "                                         'CNT_CHILDREN_CLIPPED']\n",
    "\n",
    "for f_ in rejected_features:\n",
    "    del combined[f_]\n",
    "    \n",
    "data = combined.iloc[0:train_size,:].copy(deep=True)\n",
    "test = combined.iloc[-test_size:,:].copy(deep=True)\n",
    "print(data.shape,test.shape,combined.shape)\n",
    "##### Split train and test######\n",
    "\n",
    "#Label Encoding\n",
    "categorical_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "for f_ in categorical_feats:\n",
    "    nunique = data[f_].nunique(dropna=False)\n",
    "    print(f_,nunique,data[f_].unique())\n",
    "    if (nunique<6):\n",
    "        cat_feats.append(f_)\n",
    "    else:\n",
    "        meanenc_feats.append(f_)\n",
    "    data[f_], indexer = pd.factorize(data[f_])\n",
    "    test[f_] = indexer.get_indexer(test[f_])\n",
    "\n",
    "data = downcast_dtypes(data)\n",
    "test = downcast_dtypes(test)\n",
    "\n",
    "del combined\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47c8d5a61e24759d8d80505a8a5b97526d2f954b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv('data_app.csv', index=False, compression='zip')\n",
    "#test.to_csv('test_app.csv', index=False, compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bureau balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d4be18dd8500b8075b19f9f9d44a208cfaee252"
   },
   "outputs": [],
   "source": [
    "bubl = pd.read_csv('../input/bureau_balance.csv.zip')\n",
    "\n",
    "#what is the last month with DPD\n",
    "bubl_last_DPD = bubl[bubl.STATUS.isin(['1','2','3','4','5'])].groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].max()\n",
    "bubl_last_DPD.rename('MONTH_LAST_DPD', inplace=True)\n",
    "print(bubl_last_DPD.head())\n",
    "\n",
    "#what is the last month complete\n",
    "bubl_last_C = bubl[bubl.STATUS=='C'].groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].min()\n",
    "bubl_last_C.rename('MONTH_LAST_C',inplace=True)\n",
    "print(bubl_last_C.head())\n",
    "\n",
    "STATUS_TCNT = pd.Series(bubl.groupby('SK_ID_BUREAU')['STATUS'].value_counts(), name='STATUS_TCNT')\n",
    "STATUS_TCNT = pd.pivot_table(STATUS_TCNT.reset_index(),\n",
    "                            index='SK_ID_BUREAU',columns='STATUS',values='STATUS_TCNT',fill_value=0)\n",
    "STATUS_TCNT['DPD_SUM'] = np.zeros([STATUS_TCNT.shape[0]])\n",
    "count = np.zeros([STATUS_TCNT.shape[0]])\n",
    "for i in range(0,6):\n",
    "    STATUS_TCNT['DPD_SUM'] += STATUS_TCNT[str(i)]*i\n",
    "    count += STATUS_TCNT[str(i)]\n",
    "    del STATUS_TCNT[str(i)]\n",
    "STATUS_TCNT['DPD_MEAN'] = STATUS_TCNT['DPD_SUM']/(count+0.0001)\n",
    "\n",
    "STATUS_TCNT.columns = ['STATUS_TCNT_' + f_ for f_ in STATUS_TCNT.columns]\n",
    "STATUS_12CNT = pd.Series(bubl[bubl['MONTHS_BALANCE']>=-12].groupby('SK_ID_BUREAU')['STATUS'].value_counts(), name='STATUS_6CNT')  \n",
    "STATUS_12CNT = pd.pivot_table(STATUS_12CNT.reset_index(),\n",
    "                            index='SK_ID_BUREAU',columns='STATUS',values='STATUS_6CNT',fill_value=0)\n",
    "STATUS_12CNT['DPD_SUM'] = np.zeros([STATUS_12CNT.shape[0]])\n",
    "count = np.zeros([STATUS_12CNT.shape[0]])\n",
    "for i in range(0,6):\n",
    "    STATUS_12CNT['DPD_SUM'] += STATUS_12CNT[str(i)]*i\n",
    "    count += STATUS_12CNT[str(i)]\n",
    "    del STATUS_12CNT[str(i)]\n",
    "STATUS_12CNT['DPD_MEAN'] = STATUS_12CNT['DPD_SUM']/(count+0.0001)\n",
    "STATUS_12CNT.columns = ['STATUS_12CNT_' + f_ for f_ in STATUS_12CNT.columns]\n",
    "STATUS_12CNT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bureau records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05f3c205bbc2ec99010635fe52883ae72bfe336e"
   },
   "outputs": [],
   "source": [
    "# Now take care of bureau <===peoples credit at different buro\n",
    "buro = pd.read_csv('../input/bureau.csv.zip')\n",
    "\n",
    "buro['DAYS_CREDIT_ENDDATE'].loc[buro['DAYS_CREDIT_ENDDATE'] < -40000] = np.nan\n",
    "buro['DAYS_CREDIT_UPDATE'].loc[buro['DAYS_CREDIT_UPDATE'] < -40000] = np.nan\n",
    "buro['DAYS_ENDDATE_FACT'].loc[buro['DAYS_ENDDATE_FACT'] < -40000] = np.nan\n",
    "        \n",
    "buro['AMT_DEBT_RATIO'] = buro['AMT_CREDIT_SUM_DEBT']/(1+buro['AMT_CREDIT_SUM'])\n",
    "buro['AMT_LIMIT_RATIO'] = buro['AMT_CREDIT_SUM_LIMIT']/(1+buro['AMT_CREDIT_SUM'])\n",
    "buro['AMT_SUM_OVERDUE_RATIO'] = buro['AMT_CREDIT_SUM_OVERDUE']/(1+buro['AMT_CREDIT_SUM'])\n",
    "buro['AMT_MAX_OVERDUE_RATIO'] = buro['AMT_CREDIT_MAX_OVERDUE']/(1+buro['AMT_CREDIT_SUM'])\n",
    "buro['DAYS_END_DIFF'] = buro['DAYS_ENDDATE_FACT'] - buro['DAYS_CREDIT_ENDDATE']\n",
    "###################################\n",
    "# most recent bureau info\n",
    "###################################\n",
    "\n",
    "idx = buro.groupby(['SK_ID_CURR'])['DAYS_CREDIT'].idxmax() #most recent data\n",
    "buro_recent = buro.loc[idx.values]\n",
    "buro_recent.columns = ['recent_' + f_ for f_ in buro_recent.columns]\n",
    "#Label Encoding\n",
    "categorical_feats = [\n",
    "    f for f in buro_recent.columns if buro_recent[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "for f_ in categorical_feats:\n",
    "    nunique = buro_recent[f_].nunique(dropna=False)\n",
    "    print(f_,nunique,buro_recent[f_].unique())\n",
    "    if (nunique>=3):\n",
    "        meanenc_feats.append('bureau_'+f_)\n",
    "    buro_recent[f_], indexer = pd.factorize(buro_recent[f_])\n",
    "\n",
    "del buro_recent['recent_SK_ID_BUREAU']\n",
    "buro_recent.rename(columns={'recent_SK_ID_CURR':'SK_ID_CURR'},inplace=True)\n",
    "buro_recent.set_index('SK_ID_CURR', inplace=True)\n",
    "    \n",
    "#### merge buro balance   \n",
    "for f_ in STATUS_TCNT.columns:\n",
    "    buro[f_] = buro['SK_ID_BUREAU'].map(STATUS_TCNT[f_])\n",
    "for f_ in STATUS_12CNT.columns:\n",
    "    buro[f_] = buro['SK_ID_BUREAU'].map(STATUS_12CNT[f_])\n",
    "buro['MONTH_LAST_DPD'] = buro['SK_ID_BUREAU'].map(bubl_last_DPD)\n",
    "buro['MONTH_LAST_C'] = buro['SK_ID_BUREAU'].map(bubl_last_C)\n",
    "\n",
    "#one-hot/label encoding categorical feature\n",
    "buro_cat_features = [\n",
    "    f_ for f_ in buro.columns if buro[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in buro_cat_features:\n",
    "    # buro[f_], _ = pd.factorize(buro[f_])\n",
    "    if(buro[f_].nunique(dropna=False)<=2):\n",
    "        buro[f_], indexer = pd.factorize(buro[f_])\n",
    "    else:\n",
    "        buro = pd.concat([buro, pd.get_dummies(buro[f_], prefix=f_)], axis=1)\n",
    "        del buro[f_]\n",
    "\n",
    "#agg max\n",
    "buro['DAYS_CREDIT'] = buro['DAYS_CREDIT']\n",
    "max_feats = ['MONTH_LAST_DPD', 'MONTH_LAST_C', 'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE']\n",
    "print ('max_feats',max_feats)\n",
    "max_buro = buro[max_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').max()\n",
    "max_buro.columns = ['max_' + f_ for f_ in max_buro.columns]\n",
    "\n",
    "#agg min\n",
    "min_feats = ['MONTH_LAST_DPD', 'MONTH_LAST_C', 'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE']\n",
    "print ('min_feats',min_feats)\n",
    "min_buro = buro[min_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').max()\n",
    "min_buro.columns = ['min_' + f_ for f_ in min_buro.columns]\n",
    "\n",
    "#agg mean\n",
    "avg_feats = [f_ for f_ in buro.columns.values if (f_.find('DAY')>=0)]\n",
    "print ('avg_feats',avg_feats)\n",
    "avg_buro = buro[avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').mean()\n",
    "avg_buro.columns = ['avg_' + f_ for f_ in avg_buro.columns]\n",
    "\n",
    "#agg sum\n",
    "sum_feats = [f_ for f_ in buro.columns.values if not f_ in (['SK_ID_CURR','SK_ID_BUREAU'])]\n",
    "sum_buro = buro[sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n",
    "sum_buro.columns = ['sum_' + f_ for f_ in sum_buro.columns]\n",
    "print ('sum_feats',sum_feats)\n",
    "\n",
    "#get mode for categorical features\n",
    "for cat_ in buro_cat_features:\n",
    "    print('buro_'+cat_+'_mode')\n",
    "    cols = [f_ for f_ in sum_buro.columns.values if f_.find(cat_)>=0]\n",
    "    sum_buro[cat_+'_mode'] = sum_buro[cols].idxmax(axis=1)\n",
    "    meanenc_feats.append('bureau_'+cat_+'_mode')\n",
    "    if len(cols)>=10:\n",
    "        for col in cols:\n",
    "            del sum_buro[col]\n",
    "\n",
    "#aggragate on active accounts            \n",
    "active_buro = buro.loc[buro['CREDIT_ACTIVE_Active']==1]\n",
    "active_buro['DAYS_LEFT_RATIO'] = active_buro['DAYS_CREDIT_ENDDATE']/(active_buro['DAYS_CREDIT_ENDDATE']-active_buro['DAYS_CREDIT'])\n",
    "active_buro['AMT_CREDIT_LEFT'] = active_buro['AMT_CREDIT_SUM'] * active_buro['DAYS_LEFT_RATIO']\n",
    "active_buro['AMT_CREDIT_LEFT_OVER_ANNUITY'] = active_buro['AMT_CREDIT_LEFT'] / active_buro['AMT_ANNUITY']\n",
    "active_sum_feats = [f_ for f_ in sum_feats if (f_.find('CREDIT_CURRENCY')<0)\n",
    "                    & (f_.find('CREDIT_ACTIVE')<0) & (f_.find('STATUS_')<0)\n",
    "                    & (f_.find('MONTH_')<0) & (f_.find('CREDIT_TYPE')<0)] + ['AMT_CREDIT_LEFT','AMT_CREDIT_LEFT_OVER_ANNUITY']\n",
    "print ('active_sum_feats',active_sum_feats)\n",
    "active_sum_buro = active_buro[active_sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n",
    "del active_sum_buro['DAYS_END_DIFF']\n",
    "active_sum_buro.columns = ['active_sum_' + f_ for f_ in active_sum_buro.columns]\n",
    "active_sum_buro['active_count'] = buro.loc[buro['CREDIT_ACTIVE_Active']==1].groupby('SK_ID_CURR')['SK_ID_BUREAU'].nunique()\n",
    "\n",
    "active_avg_feats = active_sum_feats + ['DAYS_LEFT_RATIO']\n",
    "print ('active_avg_feats',active_avg_feats)\n",
    "active_avg_buro = active_buro[active_avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').mean()\n",
    "del active_avg_buro['DAYS_END_DIFF']\n",
    "active_avg_buro.columns = ['active_avg_' + f_ for f_ in active_avg_buro.columns] \n",
    "\n",
    "active_avg_buro['active_AMT_DEBT_TOTAL_RATIO'] = active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum()/active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "active_avg_buro['active_AMT_LIMIT_TOTAL_RATIO'] = active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].sum()/active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "active_avg_buro['active_AMT_SUM_OVERDUE_TOTAL_RATIO'] = active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum()/active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "active_avg_buro['active_AMT_MAX_OVERDUE_TOTAL_RATIO'] = active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].sum()/active_buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "\n",
    "#merge to bureau table\n",
    "avg_buro = avg_buro.merge(min_buro, how='outer', on='SK_ID_CURR')\n",
    "avg_buro = avg_buro.merge(max_buro, how='outer', on='SK_ID_CURR')\n",
    "avg_buro = avg_buro.merge(sum_buro, how='outer', on='SK_ID_CURR')\n",
    "avg_buro = avg_buro.merge(active_sum_buro, how='outer', on='SK_ID_CURR')\n",
    "avg_buro = avg_buro.merge(active_avg_buro, how='outer', on='SK_ID_CURR')\n",
    "avg_buro = avg_buro.merge(buro_recent, how='outer', on='SK_ID_CURR')\n",
    "avg_buro['used_other_currency'] = avg_buro[['sum_CREDIT_CURRENCY_currency 2','sum_CREDIT_CURRENCY_currency 3','sum_CREDIT_CURRENCY_currency 4']].sum(axis=1)>0\n",
    "avg_buro['used_other_currency'] = avg_buro['used_other_currency'].astype('int')\n",
    "avg_buro['count'] = buro.groupby('SK_ID_CURR')['SK_ID_BUREAU'].nunique()\n",
    "avg_buro['AMT_DEBT_TOTAL_RATIO'] = buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum()/buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "avg_buro['AMT_LIMIT_TOTAL_RATIO'] = buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].sum()/buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "avg_buro['AMT_SUM_OVERDUE_TOTAL_RATIO'] = buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum()/buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "avg_buro['AMT_MAX_OVERDUE_TOTAL_RATIO'] = buro.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].sum()/buro.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum()\n",
    "\n",
    "avg_buro.columns = ['bureau_' + f_ for f_ in avg_buro.columns]\n",
    "#downcast to save space\n",
    "avg_buro = downcast_dtypes(avg_buro)\n",
    "\n",
    "del buro, sum_feats, active_sum_buro, bubl, STATUS_TCNT, STATUS_12CNT\n",
    "gc.collect()\n",
    "avg_buro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1abfb9052d3d67c295e6080af4d1af8cb07d915",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#avg_buro.to_csv('avg_buro.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### credit card balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73cd8532460a9502c3cc4d1a953b785b3ad398d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccbl = pd.read_csv('../input/credit_card_balance.csv.zip')\n",
    "cc_target1 = ccbl.SK_ID_PREV.loc[ccbl.SK_DPD_DEF>0].unique()\n",
    "\n",
    "sum_feats = [f_ for f_ in ccbl.columns.values if ((f_.find('AMT')>=0) | (f_.find('SK_DPD')>=0) | (f_.find('CNT')>=0) & (f_.find('CUM')==-1))]\n",
    "print('sum_feats',sum_feats)\n",
    "sum_ccbl_mon = ccbl.groupby(['SK_ID_CURR','MONTHS_BALANCE'])[sum_feats].sum()\n",
    "sum_ccbl_mon['CNT_ACCOUNT_W_MONTH'] = ccbl.groupby(['SK_ID_CURR','MONTHS_BALANCE'])['SK_ID_PREV'].count()\n",
    "sum_ccbl_mon = sum_ccbl_mon.reset_index()\n",
    "\n",
    "#compute ratio after summing up account\n",
    "sum_ccbl_mon['AMT_BALANCE_CREDIT_RATIO'] = (sum_ccbl_mon['AMT_BALANCE']/(sum_ccbl_mon['AMT_CREDIT_LIMIT_ACTUAL']+0.001)).clip(-100,100)\n",
    "sum_ccbl_mon['AMT_CREDIT_USE_RATIO'] = (sum_ccbl_mon['AMT_DRAWINGS_CURRENT']/(sum_ccbl_mon['AMT_CREDIT_LIMIT_ACTUAL']+0.001)).clip(-100,100)\n",
    "sum_ccbl_mon['AMT_DRAWING_ATM_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_ATM_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\n",
    "sum_ccbl_mon['AMT_DRAWINGS_OTHER_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_OTHER_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\n",
    "sum_ccbl_mon['AMT_DRAWINGS_POS_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_POS_CURRENT']/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)\n",
    "sum_ccbl_mon['AMT_PAY_USE_RATIO'] = ((sum_ccbl_mon['AMT_PAYMENT_TOTAL_CURRENT']+0.001)/(sum_ccbl_mon['AMT_DRAWINGS_CURRENT']+0.001)).clip(-100,100)\n",
    "sum_ccbl_mon['AMT_BALANCE_RECIVABLE_RATIO'] = sum_ccbl_mon['AMT_BALANCE']/(sum_ccbl_mon['AMT_TOTAL_RECEIVABLE']+0.001)\n",
    "sum_ccbl_mon['AMT_DRAWING_BALANCE_RATIO'] = sum_ccbl_mon['AMT_DRAWINGS_CURRENT']/(sum_ccbl_mon['AMT_BALANCE']+0.001)\n",
    "sum_ccbl_mon['AMT_RECEIVABLE_PRINCIPAL_DIFF'] = sum_ccbl_mon['AMT_TOTAL_RECEIVABLE']-sum_ccbl_mon['AMT_RECEIVABLE_PRINCIPAL']\n",
    "sum_ccbl_mon['AMT_PAY_INST_DIFF'] = sum_ccbl_mon['AMT_PAYMENT_CURRENT'] - sum_ccbl_mon['AMT_INST_MIN_REGULARITY']\n",
    "\n",
    "rejected_features = ['AMT_RECIVABLE','AMT_RECEIVABLE_PRINCIPAL',\n",
    "                     'AMT_DRAWINGS_OTHER_CURRENT','AMT_DRAWINGS_POS_CURRENT']\n",
    "for f_ in rejected_features:\n",
    "    del sum_ccbl_mon[f_]\n",
    "    \n",
    "sum_feats = [f_ for f_ in sum_ccbl_mon.columns.values if ((f_.find('AMT')>=0) | (f_.find('SK_DPD')>=0) | (f_.find('CNT')>=0) & (f_.find('CUM')==-1))]\n",
    "print('updated sum_feats',sum_feats)\n",
    "\n",
    "print('compute mean for different windows')\n",
    "mean4_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-4].groupby('SK_ID_CURR').mean()\n",
    "del mean4_ccbl_mon['MONTHS_BALANCE']\n",
    "mean4_ccbl_mon.columns = ['mean4_' + f_ for f_ in mean4_ccbl_mon.columns]\n",
    "\n",
    "mean12_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-12].groupby('SK_ID_CURR').mean()\n",
    "del mean12_ccbl_mon['MONTHS_BALANCE']\n",
    "mean12_ccbl_mon.columns = ['mean12_' + f_ for f_ in mean12_ccbl_mon.columns]\n",
    "\n",
    "mean36_ccbl_mon = sum_ccbl_mon.loc[sum_ccbl_mon.MONTHS_BALANCE>=-36].groupby('SK_ID_CURR').mean()\n",
    "del mean36_ccbl_mon['MONTHS_BALANCE']\n",
    "mean36_ccbl_mon.columns = ['mean36_' + f_ for f_ in mean36_ccbl_mon.columns]\n",
    "\n",
    "#sum_ccbl_mon2 for scale features\n",
    "print('compute scaled sum and mean')\n",
    "sum_ccbl_mon2 = sum_ccbl_mon.copy(deep=True)\n",
    "sum_ccbl_mon2['YEAR_SCALE'] = (sum_ccbl_mon2['MONTHS_BALANCE']/12.0).apply(np.exp)\n",
    "for f_ in sum_feats:\n",
    "    sum_ccbl_mon2[f_] = sum_ccbl_mon2[f_] * sum_ccbl_mon2['YEAR_SCALE']\n",
    "\n",
    "#scale sum\n",
    "scale_sum_ccbl_mon = sum_ccbl_mon2.groupby('SK_ID_CURR').sum()\n",
    "del scale_sum_ccbl_mon['MONTHS_BALANCE'], scale_sum_ccbl_mon['YEAR_SCALE']\n",
    "scale_sum_ccbl_mon.columns = ['scale_sum_' + f_ for f_ in scale_sum_ccbl_mon.columns]\n",
    "\n",
    "#scale mean\n",
    "year_scale_sum = sum_ccbl_mon2.groupby('SK_ID_CURR')['YEAR_SCALE'].sum()\n",
    "scale_mean_ccbl_mon = pd.DataFrame()\n",
    "for f_ in scale_sum_ccbl_mon.columns:\n",
    "    scale_mean_ccbl_mon[f_] = scale_sum_ccbl_mon[f_]/year_scale_sum\n",
    "scale_mean_ccbl_mon.columns = ['scale_mean_' + f_ for f_ in scale_mean_ccbl_mon.columns]\n",
    "\n",
    "print ('compute mean,var,max,min for all months')\n",
    "#mean\n",
    "del sum_ccbl_mon['MONTHS_BALANCE']\n",
    "mean_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR').mean()\n",
    "mean_ccbl_mon.columns = ['mean_' + f_ for f_ in mean_ccbl_mon.columns]\n",
    "#var\n",
    "var_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR').var()\n",
    "var_ccbl_mon.columns = ['var_' + f_ for f_ in var_ccbl_mon.columns]\n",
    "#max\n",
    "max_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR').max()\n",
    "max_ccbl_mon.columns = ['max_' + f_ for f_ in max_ccbl_mon.columns]\n",
    "#min\n",
    "min_ccbl_mon = sum_ccbl_mon.groupby('SK_ID_CURR')['AMT_TOTAL_RECEIVABLE','AMT_RECEIVABLE_PRINCIPAL_DIFF'].min()\n",
    "min_ccbl_mon.columns = ['min_' + f_ for f_ in min_ccbl_mon.columns]\n",
    "\n",
    "print ('find last time with DPD')\n",
    "#what is the last month with DPD\n",
    "ccbl_last_DPD = ccbl[ccbl.SK_DPD>0].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\n",
    "ccbl_last_DPD.rename('MONTH_LAST_DPD',inplace=True)\n",
    "\n",
    "#what is the last month with 7 Days Past Due\n",
    "ccbl_last_DPD7 = ccbl[ccbl.SK_DPD_DEF>7].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\n",
    "ccbl_last_DPD7.rename('MONTH_LAST_DPD7',inplace=True)\n",
    "\n",
    "#ccbl_mon = mean1_ccbl_mon.merge(mean4_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = mean4_ccbl_mon.copy(deep=True)\n",
    "ccbl_mon = ccbl_mon.merge(mean12_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(mean36_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "\n",
    "ccbl_mon = ccbl_mon.merge(scale_sum_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(scale_mean_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(mean_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(var_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(max_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(min_ccbl_mon,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon['MONTH_LAST_DPD'] = ccbl_last_DPD\n",
    "ccbl_mon['MONTH_LAST_DPD7'] = ccbl_last_DPD7\n",
    "ccbl_mon['MONTH_LAST_DPD'].loc[ccbl_mon['MONTH_LAST_DPD']==0] = np.nan\n",
    "ccbl_mon['MONTH_LAST_DPD7'].loc[ccbl_mon['MONTH_LAST_DPD7']==0] = np.nan\n",
    "\n",
    "#most recent data\n",
    "print ('extract most recent data for each customer')\n",
    "idx = ccbl.groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].idxmax()\n",
    "recent = ccbl[['SK_ID_CURR','MONTHS_BALANCE','CNT_INSTALMENT_MATURE_CUM','NAME_CONTRACT_STATUS','SK_DPD','SK_DPD_DEF']].iloc[idx.values].copy(deep=True)\n",
    "#most recent NAME_CONTRACT_STATUS for mean encoding\n",
    "recent['NAME_CONTRACT_STATUS'],indexer = pd.factorize(recent['NAME_CONTRACT_STATUS'])\n",
    "meanenc_feats.append('cc_NAME_CONTRACT_STATUS')\n",
    "recent.set_index('SK_ID_CURR',inplace=True)\n",
    "\n",
    "NAME_CONTRACT_STATUS_COUNT = pd.Series(ccbl.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].value_counts(),\n",
    "                                       name='NAME_CONTRACT_STATUS_COUNT')\n",
    "NAME_CONTRACT_STATUS_COUNT = pd.pivot_table(NAME_CONTRACT_STATUS_COUNT.reset_index(), \n",
    "               index='SK_ID_CURR', columns='NAME_CONTRACT_STATUS', values='NAME_CONTRACT_STATUS_COUNT',fill_value=0)\n",
    "recent = recent.merge(NAME_CONTRACT_STATUS_COUNT,how='outer',on='SK_ID_CURR')\n",
    "ccbl_mon = ccbl_mon.merge(recent,how='outer',on='SK_ID_CURR')\n",
    "ccbl['history_len'] = ccbl.groupby('SK_ID_CURR')['MONTHS_BALANCE'].count()\n",
    "\n",
    "#########\n",
    "ccbl_mon.fillna(0,inplace=True)\n",
    "ccbl_mon.columns = ['cc_' + f_ for f_ in ccbl_mon.columns]\n",
    "ccbl_mon = downcast_dtypes(ccbl_mon)\n",
    "\n",
    "del sum_ccbl_mon, sum_ccbl_mon2\n",
    "#del mean1_ccbl_mon\n",
    "del mean4_ccbl_mon, mean12_ccbl_mon, mean36_ccbl_mon, recent\n",
    "del scale_sum_ccbl_mon, scale_mean_ccbl_mon, mean_ccbl_mon, var_ccbl_mon, max_ccbl_mon\n",
    "del ccbl\n",
    "gc.collect()\n",
    "ccbl_mon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07b5b665d951d3d174082d647be57417d00a9ffe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ccbl_mon.to_csv('ccbl_mon.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pos cash balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc9a1d2becaa76d4a98c548b41775563e2e91e2c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = pd.read_csv('../input/POS_CASH_balance.csv.zip')\n",
    "pos_target1 = pos.SK_ID_PREV.loc[pos.SK_DPD_DEF>0].unique()\n",
    "\n",
    "#later use with prev\n",
    "idx = pos.groupby(['SK_ID_PREV'])['MONTHS_BALANCE'].idxmax() #most recent data\n",
    "pos_prev_last = pos[['SK_ID_PREV','CNT_INSTALMENT','CNT_INSTALMENT_FUTURE']].loc[idx.values]\n",
    "pos_prev_last['INSTAL_LEFT_RATIO'] = pos_prev_last['CNT_INSTALMENT_FUTURE']/(pos_prev_last['CNT_INSTALMENT'])\n",
    "pos_prev_last.set_index('SK_ID_PREV',inplace=True)\n",
    "#####\n",
    "\n",
    "idx = pos.groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].idxmax() #most recent data\n",
    "pos_recent = pos[['SK_ID_CURR','MONTHS_BALANCE','CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                  'NAME_CONTRACT_STATUS','SK_DPD','SK_DPD_DEF']].loc[idx.values]\n",
    "pos_recent['NAME_CONTRACT_STATUS'],indexer = pd.factorize(pos_recent['NAME_CONTRACT_STATUS'])\n",
    "pos_recent.set_index('SK_ID_CURR',inplace=True)\n",
    "pos_recent.columns = ['recent_' + f_ for f_ in pos_recent.columns]\n",
    "\n",
    "NAME_CONTRACT_STATUS_COUNT = pd.Series(pos.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].value_counts(),\n",
    "                                       name='NAME_CONTRACT_STATUS_COUNT')\n",
    "NAME_CONTRACT_STATUS_COUNT = pd.pivot_table(NAME_CONTRACT_STATUS_COUNT.reset_index(), \n",
    "               index='SK_ID_CURR', columns='NAME_CONTRACT_STATUS', values='NAME_CONTRACT_STATUS_COUNT',fill_value=0)\n",
    "NAME_CONTRACT_STATUS_COUNT.columns = ['NAME_CONTRACT_STATUS_CNT_' + f_ for f_ in NAME_CONTRACT_STATUS_COUNT.columns] \n",
    "\n",
    "## aggragate features\n",
    "pos['YEAR_SCALE'] = (pos['MONTHS_BALANCE']/12.0).apply(np.exp)\n",
    "pos['SK_DPD_SCALE'] = pos['SK_DPD'] * pos['YEAR_SCALE']\n",
    "pos['SK_DPD_DEF_SCALE'] = pos['SK_DPD_DEF'] * pos['YEAR_SCALE']\n",
    "\n",
    "pos_max = pos.groupby(['SK_ID_CURR'])[['SK_DPD','SK_DPD_DEF']].max()\n",
    "pos_max.columns = ['max_' + f_ for f_ in pos_max.columns]\n",
    "\n",
    "pos_mean = pos.groupby(['SK_ID_CURR'])[['SK_DPD','SK_DPD_DEF']].mean()\n",
    "pos_mean.columns = ['mean_' + f_ for f_ in pos_mean.columns]\n",
    "\n",
    "pos_sum = pos.groupby(['SK_ID_CURR'])[['SK_DPD_SCALE','SK_DPD_DEF_SCALE']].sum()\n",
    "\n",
    "pos_year_sum = pos.groupby(['SK_ID_CURR'])['YEAR_SCALE'].sum()\n",
    "pos_mean_scale = pd.DataFrame()\n",
    "for f_ in pos_sum.columns:\n",
    "    pos_mean_scale[f_] = pos_sum[f_]/pos_year_sum\n",
    "\n",
    "pos_sum.columns = ['sum_' + f_ for f_ in pos_sum.columns]\n",
    "pos_mean_scale.columns = ['mean_' + f_ for f_ in pos_mean_scale.columns]\n",
    "\n",
    "#what is the last month with DPD\n",
    "pos_last_DPD = pos[pos.SK_DPD>0].groupby(['SK_ID_CURR'])['MONTHS_BALANCE'].max()\n",
    "pos_last_DPD.rename('MONTH_LAST_DPD',inplace=True)\n",
    "\n",
    "#merge to pos table\n",
    "pos_recent = pos_recent.merge(pos_max,how='outer',on='SK_ID_CURR')\n",
    "pos_recent = pos_recent.merge(pos_mean,how='outer',on='SK_ID_CURR')\n",
    "pos_recent = pos_recent.merge(pos_sum,how='outer',on='SK_ID_CURR')\n",
    "pos_recent = pos_recent.merge(pos_mean_scale,how='outer',on='SK_ID_CURR')\n",
    "pos_recent['MONTH_LAST_DPD'] = pos_last_DPD\n",
    "pos_recent = pos_recent.merge(NAME_CONTRACT_STATUS_COUNT,how='outer',on='SK_ID_CURR')\n",
    "pos_recent['MONTH_CNT'] = pos.groupby('SK_ID_CURR')['MONTHS_BALANCE'].count()\n",
    "pos_recent['MONTH_MAX'] = pos.groupby('SK_ID_CURR')['MONTHS_BALANCE'].min()\n",
    "pos_recent['count'] = pos.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\n",
    "\n",
    "pos_recent.fillna(0,inplace=True)\n",
    "pos_recent = downcast_dtypes(pos_recent)\n",
    "pos_recent.columns = ['pos_' + f_ for f_ in pos_recent.columns]\n",
    "\n",
    "meanenc_feats.append('pos_recent_NAME_CONTRACT_STATUS')\n",
    "del pos, pos_max, pos_mean, pos_sum, pos_mean_scale\n",
    "gc.collect()\n",
    "pos_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbd1cbad91387455ba1fd3db1571646eb6cdfde7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos_recent.to_csv('pos_recent.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installment payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd603329e8614fc88c200d5e9bd449e182b24f29"
   },
   "outputs": [],
   "source": [
    "inst = pd.read_csv('../input/installments_payments.csv.zip')\n",
    "\n",
    "#later use with prev\n",
    "inst_prev_last = inst.groupby('SK_ID_PREV')['AMT_PAYMENT'].sum()\n",
    "####\n",
    "\n",
    "inst_NUM_INSTALMENT_VERSION = inst.groupby(['SK_ID_CURR'])['NUM_INSTALMENT_VERSION'].nunique()\n",
    "\n",
    "#merge payments of same month\n",
    "#maybe helpful for: inst.loc[(inst.SK_ID_PREV==1000005) & (inst.SK_ID_CURR==176456) & (inst.NUM_INSTALMENT_NUMBER==9)]\n",
    "inst['DAYS_ENTRY_PAYMENT_weighted'] = inst['DAYS_ENTRY_PAYMENT'] * inst['AMT_PAYMENT']\n",
    "inst = inst.groupby(['SK_ID_PREV','SK_ID_CURR','NUM_INSTALMENT_NUMBER']).agg({'DAYS_INSTALMENT':'mean',\n",
    "                                                                       'DAYS_ENTRY_PAYMENT_weighted':'sum',\n",
    "                                                                       'AMT_INSTALMENT':'mean',\n",
    "                                                                       'AMT_PAYMENT':'sum'})\n",
    "inst['DAYS_ENTRY_PAYMENT'] = inst['DAYS_ENTRY_PAYMENT_weighted']/inst['AMT_PAYMENT']\n",
    "inst = inst.reset_index()\n",
    "del inst['DAYS_ENTRY_PAYMENT_weighted']\n",
    "\n",
    "inst_target1 = inst.loc[(inst['DAYS_ENTRY_PAYMENT']>inst['DAYS_INSTALMENT']+1)|(inst['AMT_PAYMENT']<inst['AMT_INSTALMENT'])].SK_ID_PREV.unique()\n",
    "\n",
    "#create some new feature: how many days payment delayed? how much overpayed/underpayed?\n",
    "inst['AMT_PAYMENT_PERC'] = inst['AMT_PAYMENT'] / inst['AMT_INSTALMENT']\n",
    "inst['DPD'] = inst['DAYS_ENTRY_PAYMENT'] - inst['DAYS_INSTALMENT']\n",
    "inst['DBD'] = inst['DAYS_INSTALMENT'] - inst['DAYS_ENTRY_PAYMENT']\n",
    "inst['DPD'] = inst['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "inst['DBD'] = inst['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "inst['DPD'].fillna(30, inplace=True)\n",
    "inst['DBD'].fillna(0, inplace=True)\n",
    "inst['AMT_PAYMENT_DIFF'] = inst['AMT_INSTALMENT'] - inst['AMT_PAYMENT']\n",
    "inst['DAYS_ENTRY_PAYMENT_SCALE'] = (inst['DAYS_ENTRY_PAYMENT']/365.25).apply(np.exp)\n",
    "inst['DPD_SCALE'] = inst['DPD'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\n",
    "inst['DBD_SCALE'] = inst['DBD'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\n",
    "inst['AMT_PAYMENT_DIFF_SCALE'] = inst['AMT_PAYMENT_DIFF'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\n",
    "inst['AMT_PAYMENT_SCALE'] = inst['AMT_PAYMENT'] * inst['DAYS_ENTRY_PAYMENT_SCALE']\n",
    "\n",
    "#max\n",
    "inst_max = inst.groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT_PERC']].max()\n",
    "inst_max.columns = ['max_' + f_ for f_ in inst_max.columns]\n",
    "\n",
    "#var\n",
    "inst_var = inst.groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT_PERC']].var()\n",
    "inst_var.columns = ['var_' + f_ for f_ in inst_var.columns]\n",
    "\n",
    "#sum\n",
    "inst_sum = inst.groupby('SK_ID_CURR')[['DPD_SCALE','DBD_SCALE','AMT_PAYMENT_DIFF_SCALE','AMT_PAYMENT_SCALE']].sum()\n",
    "\n",
    "inst_day_scale_sum = inst.groupby('SK_ID_CURR')['DAYS_ENTRY_PAYMENT_SCALE'].sum()\n",
    "inst_avg_scale = pd.DataFrame()\n",
    "for f_ in inst_sum.columns:\n",
    "    inst_avg_scale[f_] = inst_sum[f_]/inst_day_scale_sum\n",
    "    \n",
    "inst_sum.columns = ['sum_' + f_ for f_ in inst_sum.columns]\n",
    "inst_avg_scale.columns = ['mean_' + f_ for f_ in inst_avg_scale.columns]\n",
    "\n",
    "inst_avg = inst.groupby('SK_ID_CURR')[['DPD','DBD','AMT_PAYMENT_DIFF','AMT_PAYMENT','AMT_PAYMENT_PERC']].mean()\n",
    "inst_avg.columns = ['mean_' + f_ for f_ in inst_avg.columns]\n",
    "\n",
    "#when is the last time late\n",
    "inst_last_late = inst[inst.DAYS_INSTALMENT < inst.DAYS_ENTRY_PAYMENT].groupby(['SK_ID_CURR'])['DAYS_INSTALMENT'].max()\n",
    "inst_last_late.rename('DAYS_LAST_LATE',inplace=True)\n",
    "\n",
    "#when is the last time underpaid\n",
    "inst_last_underpaid = inst[inst.AMT_INSTALMENT < inst.AMT_PAYMENT].groupby(['SK_ID_CURR'])['DAYS_INSTALMENT'].max()\n",
    "inst_last_underpaid.rename('DAYS_LAST_UNDERPAID',inplace=True)\n",
    "\n",
    "#merge\n",
    "inst_avg = inst_avg.merge(inst_max, on='SK_ID_CURR', how='outer')\n",
    "inst_avg = inst_avg.merge(inst_var, on='SK_ID_CURR', how='outer')\n",
    "inst_avg = inst_avg.merge(inst_sum, on='SK_ID_CURR', how='outer')\n",
    "inst_avg = inst_avg.merge(inst_avg_scale, on='SK_ID_CURR', how='outer')\n",
    "inst_avg['DAYS_LAST_LATE'] = inst_last_late\n",
    "inst_avg['DAYS_LAST_UNDERPAID'] = inst_last_underpaid\n",
    "inst_avg['N_NUM_INSTALMENT_VERSION'] = inst_NUM_INSTALMENT_VERSION\n",
    "inst_avg['AMT_PAYMENT_TOTAL_RATIO'] = inst.groupby('SK_ID_CURR')['AMT_PAYMENT'].sum()/inst.groupby('SK_ID_CURR')['AMT_INSTALMENT'].sum()\n",
    "\n",
    "inst_avg['length'] = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "inst_avg['count'] = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\n",
    "inst_avg.columns = ['inst_' + f_ for f_ in inst_avg.columns]\n",
    "inst_avg = downcast_dtypes(inst_avg)\n",
    "\n",
    "del inst, inst_sum, inst_max, inst_var\n",
    "gc.collect()\n",
    "inst_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ceabb0c0d3ac968edd6d298b56704f05e4a54b5e"
   },
   "outputs": [],
   "source": [
    "#inst_avg.to_csv('inst_avg.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### previous applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7012b1646b1a71b9cda91d02c74d93387c33d12",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev = pd.read_csv('../input/previous_application.csv.zip')\n",
    "\n",
    "prev = prev.loc[prev['FLAG_LAST_APPL_PER_CONTRACT']=='Y'] #mistake rows\n",
    "del prev['FLAG_LAST_APPL_PER_CONTRACT']\n",
    "\n",
    "for f_ in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "    prev[f_].loc[prev[f_]>360000] = np.nan\n",
    "\n",
    "#create some features\n",
    "prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "prev['AMT_DIFF_CREAPP'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "prev['AMT_DIFF_CREDIT_GOODS'] = prev['AMT_CREDIT'] - prev['AMT_GOODS_PRICE']\n",
    "prev['AMT_CREDIT_GOODS_PERC'] = prev['AMT_CREDIT'] / prev['AMT_GOODS_PRICE']\n",
    "prev['AMT_PAY_YEAR'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "prev['DAYS_TOTAL'] = prev['DAYS_LAST_DUE'] - prev['DAYS_FIRST_DUE']\n",
    "prev['DAYS_TOTAL2'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_FIRST_DUE']\n",
    "prev['DAYS_END_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "prev['CNT_PAYMENT_DIFF'] = prev['CNT_PAYMENT'] - prev['SK_ID_PREV'].map(pos_prev_last['CNT_INSTALMENT'])\n",
    "\n",
    "prev['DEFAULTED'] = 0\n",
    "prev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(inst_target1)] = 1\n",
    "prev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(pos_target1)] = 1\n",
    "prev['DEFAULTED'].loc[prev['SK_ID_PREV'].isin(cc_target1)] = 1\n",
    "prev['DEFAULTED'].loc[prev['NAME_CONTRACT_STATUS']!='Approved'] = np.nan\n",
    "\n",
    "#these features highly correlated with others or not useful?\n",
    "rejected_features = ['AMT_GOODS_PRICE',\n",
    "                     'WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START',\n",
    "                     'NFLAG_LAST_APPL_IN_DAY']\n",
    "for f_ in rejected_features:\n",
    "    del prev[f_]\n",
    "    \n",
    "\n",
    "###################################\n",
    "# most recent application\n",
    "###################################\n",
    "\n",
    "idx = prev.groupby(['SK_ID_CURR'])['DAYS_DECISION'].idxmax() #most recent data\n",
    "prev_recent = prev.loc[idx.values]\n",
    "prev_recent.columns = ['recent_' + f_ for f_ in prev_recent.columns]\n",
    "#Label Encoding\n",
    "categorical_feats = [\n",
    "    f for f in prev_recent.columns if prev_recent[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "for f_ in categorical_feats:\n",
    "    nunique = prev_recent[f_].nunique(dropna=False)\n",
    "    print(f_,nunique,prev_recent[f_].unique())\n",
    "    if (nunique<5):\n",
    "        cat_feats.append('prev_'+f_)\n",
    "    else:\n",
    "        meanenc_feats.append('prev_'+f_)\n",
    "    prev_recent[f_], indexer = pd.factorize(prev_recent[f_])\n",
    "\n",
    "del prev_recent['recent_SK_ID_PREV']\n",
    "prev_recent.rename(columns={'recent_SK_ID_CURR':'SK_ID_CURR'},inplace=True)\n",
    "prev_recent.set_index('SK_ID_CURR', inplace=True)\n",
    "\n",
    "###################################\n",
    "# Changed categorical feature treatment to dummies\n",
    "# In this way averaging means something\n",
    "################################### \n",
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    if(prev[f_].nunique(dropna=False)<=2):\n",
    "        prev[f_], indexer = pd.factorize(prev[f_])\n",
    "    else:\n",
    "        prev = pd.concat([prev, pd.get_dummies(prev[f_], prefix=f_)], axis=1)\n",
    "        del prev[f_]\n",
    "################################### \n",
    "\n",
    "avg_feats = [f_ for f_ in prev.columns.values if (f_.find('DAYS')>=0) | (f_.find('RATE')>=0) | (f_.find('AMT')>=0)]\n",
    "print ('avg_feats',avg_feats)\n",
    "for f_ in avg_feats:\n",
    "    prev[f_].loc[prev[f_]>300000] = np.nan\n",
    "avg_prev = prev[avg_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').mean()\n",
    "avg_prev.columns = ['avg_' + f_ for f_ in avg_prev.columns]\n",
    "\n",
    "max_feats = [f_ for f_ in prev.columns.values if (f_.find('DAYS')>=0) | (f_.find('AMT')>=0)]\n",
    "print('max_feats',max_feats)\n",
    "max_prev = prev[max_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').max()\n",
    "max_prev.columns = ['max_' + f_ for f_ in max_prev.columns]\n",
    "\n",
    "min_feats = ['DAYS_DECISION']\n",
    "print('min_feats',min_feats)\n",
    "min_prev = prev[min_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').min()\n",
    "min_prev.columns = ['min_' + f_ for f_ in min_prev.columns]\n",
    "\n",
    "#exclude id, days, ratio for sum\n",
    "nosum_feats = ['SK_ID_CURR','SK_ID_PREV','DAYS_TOTAL','DAYS_TOTAL2','DAYS_FIRST_DRAWING',\n",
    "               'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION'\n",
    "               'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED',\n",
    "               'AMT_CREDIT_GOODS_PERC','APP_CREDIT_PERC']\n",
    "sum_feats = [f_ for f_ in prev.columns.values if not f_ in nosum_feats]\n",
    "print ('sum_feats',sum_feats)\n",
    "sum_prev = prev[sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n",
    "sum_prev.columns = ['sum_' + f_ for f_ in sum_prev.columns]\n",
    "\n",
    "\n",
    "for cat_ in prev_cat_features:\n",
    "    print('prev_'+cat_+'_mode')\n",
    "    cols = [f_ for f_ in sum_prev.columns.values if f_.find(cat_)>=0]\n",
    "    sum_prev[cat_+'_mode'] = sum_prev[cols].idxmax(axis=1)\n",
    "    meanenc_feats.append('prev_'+cat_+'_mode')\n",
    "    if len(cols)>=10:\n",
    "        for col in cols:\n",
    "            del sum_prev[col]\n",
    "\n",
    "prev_active = prev.loc[(prev['DAYS_LAST_DUE'].isnull()) & (prev['DAYS_LAST_DUE_1ST_VERSION']>0)]\n",
    "prev_active['AMT_LEFT'] = prev_active['AMT_ANNUITY'] * prev_active['DAYS_LAST_DUE_1ST_VERSION']/365.25\n",
    "prev_active['AMT_PAID'] = prev_active['SK_ID_PREV'].map(inst_prev_last)\n",
    "prev_active['AMT_OWE'] = (prev_active['AMT_CREDIT'] - prev_active['AMT_DOWN_PAYMENT'].fillna(0)) * (1+prev_active['RATE_INTEREST_PRIVILEGED'].fillna(0))\n",
    "prev_active['AMT_LEFT2'] = (prev_active['AMT_OWE']  - prev_active['AMT_PAID']).clip(lower=0)\n",
    "prev_active['LEFT_RATIO'] = prev_active['SK_ID_PREV'].map(pos_prev_last['INSTAL_LEFT_RATIO'])\n",
    "prev_active['AMT_LEFT3'] = prev_active['AMT_CREDIT'] * prev_active['LEFT_RATIO']\n",
    "prev_active['AMT_PAY_YEAR_LEFT'] = prev_active['AMT_LEFT'] / prev_active['AMT_ANNUITY']\n",
    "active_sum_feats = [f_ for f_ in prev_active.columns.values if (f_.find('AMT')>=0)]\n",
    "active_sum_prev = prev_active[active_sum_feats+['SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n",
    "active_sum_prev.columns = ['active_sum_' + f_ for f_ in active_sum_prev.columns]\n",
    "active_sum_prev['active_count'] = prev_active[['SK_ID_PREV','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_PREV']\n",
    "print ('active_sum_feats',active_sum_feats)\n",
    "\n",
    "num_aggregations = {\n",
    "    'SK_ID_PREV': ['count'],\n",
    "    'AMT_ANNUITY': [ 'max', 'mean'],\n",
    "    'AMT_APPLICATION': [ 'max','mean'],\n",
    "    'AMT_CREDIT': [ 'mean', 'sum'],\n",
    "    'APP_CREDIT_PERC': [ 'max', 'mean'],\n",
    "    'AMT_DIFF_CREAPP': [ 'max', 'mean'],\n",
    "    'AMT_DIFF_CREDIT_GOODS': [ 'max', 'mean'],\n",
    "    'AMT_CREDIT_GOODS_PERC': [ 'max', 'mean'],\n",
    "    'AMT_PAY_YEAR': [ 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "    'DAYS_DECISION': [ 'max', 'mean', 'min'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "}\n",
    "print('aggragate with in approved and refused applications')\n",
    "approved_prev = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1].groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "approved_prev.columns = pd.Index(['approved_' + e[0] + \"_\" + e[1].upper() for e in approved_prev.columns.tolist()])\n",
    "refused_prev = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1].groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "refused_prev.columns = pd.Index(['refused_' + e[0] + \"_\" + e[1].upper() for e in refused_prev.columns.tolist()])\n",
    "\n",
    "print('find one with closest annuity/credit')\n",
    "tmp = pd.concat([data[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY']], test[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY']]], axis=0).set_index('SK_ID_CURR')\n",
    "prev['AMT_CREDIT_DIFF'] = (prev['AMT_CREDIT'] - prev['SK_ID_CURR'].map(tmp['AMT_CREDIT'])).abs()\n",
    "prev['AMT_ANNUITY_DIFF'] = (prev['AMT_ANNUITY'] - prev['SK_ID_CURR'].map(tmp['AMT_ANNUITY'])).abs()\n",
    "\n",
    "idx = prev.groupby('SK_ID_CURR')['AMT_CREDIT_DIFF'].idxmin()\n",
    "idx = idx.loc[~idx.isnull()]\n",
    "prev_closest_credit_defaulted = prev[['SK_ID_CURR','DEFAULTED']].loc[idx].set_index('SK_ID_CURR')\n",
    "prev_closest_credit_defaulted.rename({'DEFAULTED':'closest_credit_defaulted'},axis=1,inplace=True)\n",
    "\n",
    "idx = prev.groupby('SK_ID_CURR')['AMT_ANNUITY_DIFF'].idxmin()\n",
    "idx = idx.loc[~idx.isnull()]\n",
    "prev_closest_annuity_defaulted = prev[['SK_ID_CURR','DEFAULTED']].loc[idx].set_index('SK_ID_CURR')\n",
    "prev_closest_annuity_defaulted.rename({'DEFAULTED':'closest_annuity_defaulted'},axis=1,inplace=True)\n",
    "\n",
    "#merge...\n",
    "print('merge')\n",
    "avg_prev = avg_prev.merge(max_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(sum_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(min_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(active_sum_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(approved_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(refused_prev, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(prev_recent, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(prev_closest_credit_defaulted, on='SK_ID_CURR', how='outer')\n",
    "avg_prev = avg_prev.merge(prev_closest_annuity_defaulted, on='SK_ID_CURR', how='outer')\n",
    "avg_prev['count'] = prev[['SK_ID_PREV','SK_ID_CURR']].groupby('SK_ID_CURR')['SK_ID_PREV'].count()\n",
    "avg_prev['DEFALUTED_RATIO'] = prev[['SK_ID_CURR','DEFAULTED']].groupby('SK_ID_CURR')['DEFAULTED'].mean()\n",
    "\n",
    "avg_prev.columns = ['prev_' + f_ for f_ in avg_prev.columns]\n",
    "\n",
    "avg_prev = downcast_dtypes(avg_prev)\n",
    "del prev, prev_recent, sum_prev, active_sum_prev\n",
    "del approved_prev, refused_prev\n",
    "gc.collect()\n",
    "avg_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9563ba9a5ea4047f625f5cdba4d64f7513e5f3c4"
   },
   "outputs": [],
   "source": [
    "#avg_prev.to_csv('avg_prev.csv', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all data to main application table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72b82e8215c11c6294833e28771286b51b7d111f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now merge all the data\n",
    "data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=ccbl_mon.reset_index(), how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=pos_recent.reset_index(), how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=inst_avg.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=ccbl_mon.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=pos_recent.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=inst_avg.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "del avg_prev, avg_buro, ccbl_mon, pos_recent, inst_avg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read aggragated month score from disk (generated by month-training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "839731eedd60d55e78affb5e1cae80c6a52fd4f7"
   },
   "outputs": [],
   "source": [
    "cols = ['SK_ID_CURR','month_score_max','month_score_std','month_score_mean','month_score_sum']\n",
    "agg_month_score = pd.read_csv('../output/agg_month_score.csv',usecols=cols,compression='zip')\n",
    "data = data.merge(right=agg_month_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=agg_month_score, how='left', on='SK_ID_CURR')\n",
    "del agg_month_score\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read aggragated prev/bureau score from disk (generated by prev-training.ipynb and buro-training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f9e9ab6f60d2369062088d604326eb93bc2ab78",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_prev_score = pd.read_csv('../output/agg_prev_score.csv', compression='zip')\n",
    "agg_buro_score = pd.read_csv('../output/agg_buro_score.csv', compression='zip')\n",
    "del agg_prev_score['TARGET']\n",
    "del agg_buro_score['TARGET']\n",
    "gc.collect()\n",
    "data = data.merge(right=agg_prev_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=agg_prev_score, how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=agg_buro_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=agg_buro_score, how='left', on='SK_ID_CURR')\n",
    "del agg_prev_score, agg_buro_score\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read house and doc features (generated by house-doc-feat.pynb)\n",
    "read time series features for bureau balance, pos-cash, installment and credit card (generated by *_ts.pynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6668513f5bd81537bcb17e6964f6c42c97605498",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_house_score = pd.read_csv('../output/train_house_score.csv')\n",
    "test_house_score = pd.read_csv('../output/test_house_score.csv')\n",
    "house_score_ext = pd.read_csv('../ourput/house_ex.csv')\n",
    "\n",
    "data = data.merge(right=train_house_score, how='left', on='SK_ID_CURR')\n",
    "data = data.merge(right=house_score_ext, how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=test_house_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=house_score_ext, how='left', on='SK_ID_CURR')\n",
    "\n",
    "train_cc_score = pd.read_csv('../ouput/cc_score_train.csv')\n",
    "test_cc_score = pd.read_csv('../ouput/cc_score_test.csv')\n",
    "data = data.merge(right=train_cc_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=test_cc_score, how='left', on='SK_ID_CURR')\n",
    "\n",
    "train_bubl_score = pd.read_csv('../ouput/bubl_score_train.csv')\n",
    "test_bubl_score = pd.read_csv('../ouput/bubl_score_test.csv')\n",
    "data = data.merge(right=train_bubl_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=test_bubl_score, how='left', on='SK_ID_CURR')\n",
    "\n",
    "train_pos_score = pd.read_csv('../ouput/pos_score_train.csv')\n",
    "test_pos_score = pd.read_csv('../ouput/pos_score_test.csv')\n",
    "data = data.merge(right=train_pos_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=test_pos_score, how='left', on='SK_ID_CURR')\n",
    "\n",
    "train_inst_score = pd.read_csv('../ouput/inst_score_train.csv')\n",
    "test_inst_score = pd.read_csv('../ouput/inst_score_test.csv')\n",
    "data = data.merge(right=train_inst_score, how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=test_inst_score, how='left', on='SK_ID_CURR')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some more features from merged table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17189a223fa848323ef281030c3ca40e63b70c0a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#more feature after merge\n",
    "data['Total_AMT_ANNUITY'] = data[['AMT_ANNUITY','bureau_active_sum_AMT_ANNUITY','prev_active_sum_AMT_ANNUITY']].sum(axis=1)\n",
    "data['Total_ANNUITY_INCOME_RATIO'] = data['Total_AMT_ANNUITY'] / data['AMT_INCOME_TOTAL']\n",
    "data['Total_CREDIT'] = data[['AMT_CREDIT','prev_active_sum_AMT_LEFT']].sum(axis=1) #exclude AMT already paid\n",
    "data['Total_CREDIT_INCOME_RATIO'] = data['Total_CREDIT'] / data['AMT_INCOME_TOTAL']\n",
    "data['Total_acc'] = data[['prev_count','bureau_count']].sum(axis=1)\n",
    "data['Total_active_acc'] = data[['prev_active_count','bureau_active_count']].sum(axis=1)\n",
    "data['Total_AMT_LEFT'] = data['AMT_CREDIT'] + data['prev_active_sum_AMT_LEFT'] + data['bureau_active_sum_AMT_CREDIT_LEFT']\n",
    "data['Total_AMT_LEFT_INCOME_RATIO'] = data['Total_AMT_LEFT']/data['AMT_INCOME_TOTAL']\n",
    "\n",
    "test['Total_AMT_ANNUITY'] = test[['AMT_ANNUITY','bureau_active_sum_AMT_ANNUITY','prev_active_sum_AMT_ANNUITY']].sum(axis=1)\n",
    "test['Total_ANNUITY_INCOME_RATIO'] = test['Total_AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "test['Total_CREDIT'] = test[['AMT_CREDIT','prev_active_sum_AMT_LEFT']].sum(axis=1)\n",
    "test['Total_CREDIT_INCOME_RATIO'] = test['Total_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "test['Total_acc'] = test[['prev_count','bureau_count']].sum(axis=1)\n",
    "test['Total_active_acc'] = test[['prev_active_count','bureau_active_count']].sum(axis=1)\n",
    "test['Total_AMT_LEFT'] = test['AMT_CREDIT'] + test['prev_active_sum_AMT_LEFT'] + test['bureau_active_sum_AMT_CREDIT_LEFT']\n",
    "test['Total_AMT_LEFT_INCOME_RATIO'] = test['Total_AMT_LEFT']/test['AMT_INCOME_TOTAL']\n",
    "\n",
    "shared_feats = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_PAY_YEAR', \n",
    "                'AMT_DIFF_CREDIT_GOODS', 'AMT_CREDIT_GOODS_PERC']\n",
    "for f_ in shared_feats:\n",
    "    data[f_+'_to_prev_approved'] = (data[f_] - data['prev_approved_'+f_+'_MEAN'])/data['prev_approved_'+f_+'_MEAN']\n",
    "    data[f_+'_to_prev_refused'] = (data[f_] - data['prev_refused_'+f_+'_MEAN'])/data['prev_refused_'+f_+'_MEAN']\n",
    "    test[f_+'_to_prev_approved'] = (test[f_] - test['prev_approved_'+f_+'_MEAN'])/test['prev_approved_'+f_+'_MEAN']\n",
    "    test[f_+'_to_prev_refused'] = (test[f_] - test['prev_refused_'+f_+'_MEAN'])/test['prev_refused_'+f_+'_MEAN']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many features we have now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ce4cb098166bbed0775f10f23480c2b1985b7b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('num of features:',data.shape[1])\n",
    "print('meanenc_feats')\n",
    "meanenc_feats = list(set(meanenc_feats))\n",
    "print(len(meanenc_feats), meanenc_feats)\n",
    "print('categorical_feats')\n",
    "print(len(cat_feats), cat_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a819267a1ab8fce77b2806f81f0cdef7e261aff7",
    "collapsed": true
   },
   "source": [
    "#### Modeling\n",
    "\n",
    "Define bagging classifer. As the target is unbalance --- defaulted accounts (target=1) count for 1/10 of whole training set, we will down sample the majority class (target=0).\n",
    "\n",
    "In each fold, the majority class is divided into 3, and we bag the three runs, in each run the whole mininority class is trained with 1/3 majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "453db7c837e2445e355069967347d1cb3a6e9300",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class bagging_classifier:\n",
    "\n",
    "    def __init__(self, base_estimator, n_estimators):\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        self.n_estimators_ = n_estimators\n",
    "\n",
    "    def fit(self, X, y, eval_set = None, eval_metric = None, verbose = None, early_stopping_rounds = None, categorical_feature = None):\n",
    "        \n",
    "        self.estimators_ = []\n",
    "        self.feature_importances_gain_ = np.zeros(X.shape[1])\n",
    "        self.feature_importances_split_ = np.zeros(X.shape[1])\n",
    "        self.n_classes_ = y.nunique()\n",
    "\n",
    "        if self.n_estimators_ == 1:\n",
    "            print ('n_estimators=1, no downsampling')\n",
    "            estimator = deepcopy(self.base_estimator_)\n",
    "            estimator.fit(X, y, eval_set = [(X, y)] + eval_set,\n",
    "                eval_metric = eval_metric, verbose = verbose, \n",
    "                early_stopping_rounds = early_stopping_rounds)\n",
    "            self.estimators_.append(estimator)\n",
    "            self.feature_importances_gain_ += estimator.booster_feature_importance(importance_type='gain')\n",
    "            self.feature_importances_split_ += estimator.booster_feature_importance(importance_type='split')\n",
    "            return\n",
    "\n",
    "    #average down sampling results\n",
    "        minority = y.value_counts().sort_values().index.values[0]\n",
    "        majority = y.value_counts().sort_values().index.values[1]\n",
    "        print('majority class:', majority)\n",
    "        print('minority class:', minority)\n",
    "\n",
    "        X_min = X.loc[y==minority]\n",
    "        y_min = y.loc[y==minority]\n",
    "        X_maj = X.loc[y==majority]\n",
    "        y_maj = y.loc[y==majority]\n",
    "\n",
    "        kf = KFold(self.n_estimators_, shuffle=True, random_state=42)\n",
    "\n",
    "        for rest, this in kf.split(y_maj):\n",
    "\n",
    "            print('training on a subset')\n",
    "            X_maj_sub = X_maj.iloc[this]\n",
    "            y_maj_sub = y_maj.iloc[this]\n",
    "            X_sub = pd.concat([X_min, X_maj_sub])\n",
    "            y_sub = pd.concat([y_min, y_maj_sub])\n",
    "\n",
    "            estimator = deepcopy(self.base_estimator_)\n",
    "\n",
    "            estimator.fit(X_sub, y_sub, eval_set = [(X_sub, y_sub)] + eval_set,\n",
    "                eval_metric = eval_metric, verbose = verbose, \n",
    "                early_stopping_rounds = early_stopping_rounds,\n",
    "                categorical_feature = categorical_feature)\n",
    "\n",
    "            self.estimators_.append(estimator)\n",
    "            self.feature_importances_gain_ += estimator.booster_.feature_importance(importance_type='gain')/self.n_estimators_\n",
    "            self.feature_importances_split_ += estimator.booster_.feature_importance(importance_type='split')/self.n_estimators_\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        proba = np.zeros([n_samples, self.n_classes_])\n",
    "\n",
    "        for estimator in self.estimators_:\n",
    "\n",
    "            proba += estimator.predict_proba(X, num_iteration=estimator.best_iteration_)/self.n_estimators_\n",
    "\n",
    "        return proba\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b29c6a0d02a2477d7b06e28928c49cdf53ce7a22",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Serious training....\n",
    "# Get features\n",
    "excluded_feats = ['SK_ID_CURR','TARGET'] + ['prev_sum_CODE_REJECT_REASON_CLIENT','bureau_sum_CREDIT_ACTIVE_Active']\n",
    "print(excluded_feats)\n",
    "\n",
    "y = data['TARGET']\n",
    "\n",
    "# Run a 5 fold\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "oof_preds = np.zeros(data.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data,data['TARGET'])):\n",
    "    trn, val = data.iloc[trn_idx], data.iloc[val_idx]\n",
    "\n",
    "#combine val and test to be encoded\n",
    "    val_test = pd.concat([val,test],axis=0,sort=False)\n",
    "    val_size = val.shape[0]\n",
    "    test_size = test.shape[0]\n",
    "    print ('doing mean_encoding')\n",
    "    trn, val_test = mean_encode(trn, val_test, meanenc_feats, 'TARGET', drop=True)\n",
    "    features = [f_ for f_ in trn.columns if f_ not in excluded_feats]\n",
    "    \n",
    "    val  = val_test.iloc[0:val_size, :].copy(deep=True)\n",
    "    test_x = val_test[features].iloc[-test_size:,:].copy(deep=True)\n",
    "        \n",
    "    trn_x, trn_y = trn[features], trn['TARGET']\n",
    "    val_x, val_y = val[features], val['TARGET']\n",
    "    print('done mean_encoding')\n",
    "    \n",
    "#    clf = LGBMClassifier(\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=26,\n",
    "        metric='auc',\n",
    "        colsample_bytree=0.3,\n",
    "        subsample=0.9320,\n",
    "        max_depth=4,\n",
    "        reg_alpha=4.8299,\n",
    "        reg_lambda=3.6335,\n",
    "        min_split_gain=0.0068,\n",
    "        min_child_weight=9.8138,\n",
    "        silent=True,\n",
    "        verbose=-1,\n",
    "        n_jobs = 16,\n",
    "        random_state = n_fold * 619,\n",
    "        class_weight = {0:1,1:1.0122}\n",
    "    )\n",
    "    \n",
    "    clf = bagging_classifier(model, 3)\n",
    "\n",
    "    clf.fit(trn_x, trn_y, \n",
    "            eval_set= [(val_x, val_y)], \n",
    "            eval_metric='auc', verbose=100, early_stopping_rounds=100,\n",
    "            categorical_feature = cat_feats,\n",
    "           )\n",
    "    \n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test_x)[:, 1] / folds.n_splits\n",
    "    \n",
    "    fold_score = roc_auc_score(val_y, oof_preds[val_idx])\n",
    "    scores.append(fold_score)\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, fold_score))\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance_gain\"] = clf.feature_importances_gain_\n",
    "    fold_importance_df[\"importance_split\"] = clf.feature_importances_split_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "    del trn, val\n",
    "    gc.collect()\n",
    "    \n",
    "print('Full AUC score %.6f +- %0.4f' % (roc_auc_score(y, oof_preds), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions and save to disk for blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af9454b1017c76709029ade36676ec841166b0aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame({'SK_ID_CURR': data['SK_ID_CURR'], 'prob': oof_preds, 'target': data['TARGET']})\n",
    "sns.distplot(prob_df['prob'].loc[prob_df.target==1] , color=\"skyblue\", label=\"target 1\")\n",
    "sns.distplot(prob_df['prob'].loc[prob_df.target==0] , color=\"red\", label=\"target 0\")\n",
    "plt.legend()\n",
    "prob_df.to_csv('../output/train_pred_lgb1.csv',index=False)\n",
    "\n",
    "test['TARGET'] = sub_preds\n",
    "test[['SK_ID_CURR', 'TARGET']].to_csv('../output/test_pred_lgb1.csv', index=False, float_format='%.8f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance and auc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "193b29cda910cbc9f413062ee7f85210494d14c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "feature_importance = feature_importance_df[[\"feature\", \"importance_gain\", \"importance_split\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"importance_split\", ascending=False)\n",
    "feature_importance.to_csv('../output/lgb1_feature_importance.csv')\n",
    "\n",
    "best_features = feature_importance.iloc[:50].reset_index()\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='importance_split', y='feature', data=best_features, ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='importance_gain', y='feature', data=best_features, ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\n",
    "fig.subplots_adjust(top=0.93)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(6,6))\n",
    "scores = [] \n",
    "for n_fold, (_, val_idx) in enumerate(folds.split(data, data['TARGET'])):  \n",
    "    # Plot the roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y.iloc[val_idx], oof_preds[val_idx])\n",
    "    score = roc_auc_score(y.iloc[val_idx], oof_preds[val_idx])\n",
    "    scores.append(score)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\n",
    "fpr, tpr, thresholds = roc_curve(y, oof_preds)\n",
    "score = roc_auc_score(y, oof_preds)\n",
    "plt.plot(fpr, tpr, color='b',\n",
    "         label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LightGBM ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
